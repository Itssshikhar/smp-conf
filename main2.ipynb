{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: torch in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/shikhar2807/miniconda3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "libcublas.so.*[0-9] not found in the system path ['/home/shikhar2807/coding-mess/conference-alpha', '', '/home/shikhar2807/workspaces/ROS/devel/lib/python3/dist-packages', '/opt/ros/noetic/lib/python3/dist-packages', '/home/shikhar2807/miniconda3/lib/python310.zip', '/home/shikhar2807/miniconda3/lib/python3.10', '/home/shikhar2807/miniconda3/lib/python3.10/lib-dynload', '/home/shikhar2807/miniconda3/lib/python3.10/site-packages', '/home/shikhar2807/coding-mess/transformers/src', '/home/shikhar2807/workspaces/owl_robot_client', '/home/shikhar2807/miniconda3/lib/python3.10/site-packages/rerun_sdk', '/opt/ros/noetic/lib/python3/dist-packages/setuptools/_vendor']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/__init__.py:226\u001b[0m, in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_deps_lib_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRTLD_GLOBAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# Can only happen for wheel with cuda libs as PYPI deps\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# As PyTorch is not purelib, but nvidia-*-cu12 is\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: libcudart.so.12: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/__init__.py:289\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# Easy way.  You want this most of the time, because it will prevent\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# C++ symbols from libtorch clobbering C++ symbols from other\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# See Note [Global dependencies]\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[0;32m--> 289\u001b[0m         \u001b[43m_load_global_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/__init__.py:247\u001b[0m, in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lib_folder, lib_name \u001b[38;5;129;01min\u001b[39;00m cuda_libs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 247\u001b[0m         \u001b[43m_preload_cuda_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlib_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mCDLL(global_deps_lib_path, mode\u001b[38;5;241m=\u001b[39mctypes\u001b[38;5;241m.\u001b[39mRTLD_GLOBAL)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m library_path:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# loading libtorch_global_deps first due its special logic\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/__init__.py:169\u001b[0m, in \u001b[0;36m_preload_cuda_deps\u001b[0;34m(lib_folder, lib_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_path:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlib_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the system path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m ctypes\u001b[38;5;241m.\u001b[39mCDLL(lib_path)\n",
      "\u001b[0;31mValueError\u001b[0m: libcublas.so.*[0-9] not found in the system path ['/home/shikhar2807/coding-mess/conference-alpha', '', '/home/shikhar2807/workspaces/ROS/devel/lib/python3/dist-packages', '/opt/ros/noetic/lib/python3/dist-packages', '/home/shikhar2807/miniconda3/lib/python310.zip', '/home/shikhar2807/miniconda3/lib/python3.10', '/home/shikhar2807/miniconda3/lib/python3.10/lib-dynload', '/home/shikhar2807/miniconda3/lib/python3.10/site-packages', '/home/shikhar2807/coding-mess/transformers/src', '/home/shikhar2807/workspaces/owl_robot_client', '/home/shikhar2807/miniconda3/lib/python3.10/site-packages/rerun_sdk', '/opt/ros/noetic/lib/python3/dist-packages/setuptools/_vendor']"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('extensive_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data = pd.read_csv('extensive_dataset.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = data.iloc[:, 7:14]\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.columns = ['Strain Dynamic (%)', 'Stress Dynamic (MPa)', 'Time (sec)', 'freq (Hz)', 'Temp (C)', \"E' (MPa)\", \"E'' (MPa)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['Strain Dynamic (%)'] = pd.to_numeric(cleaned_data['Strain Dynamic (%)'], errors='coerce')\n",
    "cleaned_data['Stress Dynamic (MPa)'] = pd.to_numeric(cleaned_data['Stress Dynamic (MPa)'], errors='coerce')\n",
    "cleaned_data[\"Time (sec)\"] = pd.to_numeric(cleaned_data[\"Time (sec)\"], errors='coerce')\n",
    "cleaned_data[\"freq (Hz)\"] = pd.to_numeric(cleaned_data[\"freq (Hz)\"], errors='coerce')\n",
    "cleaned_data['Temp (C)'] = pd.to_numeric(cleaned_data['Temp (C)'], errors='coerce')\n",
    "cleaned_data[\"E' (MPa)\"] = pd.to_numeric(cleaned_data[\"E' (MPa)\"], errors='coerce')\n",
    "cleaned_data[\"E'' (MPa)\"] = pd.to_numeric(cleaned_data[\"E'' (MPa)\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.dropna(how='all', inplace=True)\n",
    "cleaned_data.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned_data[['Strain Dynamic (%)', 'Stress Dynamic (MPa)', 'Time (sec)', 'freq (Hz)', 'Temp (C)']].values\n",
    "y = cleaned_data[[\"E' (MPa)\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the correlation of column A against all others\n",
    "corr_matrix = cleaned_data.corr()[\"E' (MPa)\"]\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).unsqueeze(2) # unsqueeze for CNN, do not need for FFNN\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).unsqueeze(2) # unsqueeze for CNN, do not need for FFNN\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Normal CNN just working with some extra functions\n",
    "\n",
    "# Define the deeper CNN model with BatchNorm, Dropout, and Xavier Initialization\n",
    "class DeeperCNNWithBNDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeeperCNNWithBNDropout, self).__init__()\n",
    "\n",
    "        # First block of convolutions\n",
    "        self.conv1 = nn.Conv1d(in_channels=5, out_channels=32, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        # Second block of convolutions\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.conv4 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=1)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=1)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        # Fully connected layers (fc1 input size will be set dynamically)\n",
    "        self.fc1 = None  # Placeholder, will be set dynamically in forward\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "        # Apply Xavier initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Second block\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten to [batch_size, features]\n",
    "\n",
    "        # Dynamically define fc1 based on the flattened size of x\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(x.size(1), 256).to(x.device)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # Initialize the weights of the model using Xavier initialization\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "# Instantiate the model\n",
    "model = DeeperCNNWithBNDropout()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Normal Feed Forward Neural Network that works with SmoothL1Loss instead of MSE\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "       # Input layer (taking 5 inputs: Stress, Strain, Freq, Time, Temp)\n",
    "        self.fc1 = nn.Linear(5, 64)  # 5 inputs, 64 neurons in the first layer\n",
    "        self.fc2 = nn.Linear(64, 128)  # 64 neurons in the second layer, 128 in the next\n",
    "        self.fc3 = nn.Linear(128, 1)  # 128 neurons to 1 output (E')\n",
    "\n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure x is flattened to shape [batch_size, input_size]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))  # Apply first layer and ReLU activation\n",
    "        x = self.dropout(x)  # Dropout\n",
    "        x = self.relu(self.fc2(x))  # Apply second layer and ReLU activation\n",
    "        x = self.dropout(x)  # Dropout\n",
    "        x = self.fc3(x)  # Output layer (no activation for regression)\n",
    "        return x.view(-1, 1, 1)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "print(f\"This is the shape of Input size: {input_size}\")\n",
    "model = FeedForwardNN(input_size)\n",
    "\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Define the CNN + LSTM Hybrid Model\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=5, out_channels=32, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout_cnn = nn.Dropout(0.3)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=64, num_layers=1, batch_first=True)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN layers\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout_cnn(x)\n",
    "\n",
    "        # Prepare the input for LSTM by permuting to [batch_size, sequence_length, features]\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # LSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        # Take the last output of the LSTM\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x.view(-1, 1)\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNNLSTM()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Defining the FNN(Feed Forward Neural Network) + Transformer\n",
    "class FNNTransformerHybrid(nn.Module):\n",
    "    def __init__(self, input_size, transformer_hidden_size, num_heads, transformer_layers):\n",
    "        super(FNNTransformerHybrid, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # First feedforward layer\n",
    "        self.fc2 = nn.Linear(64, 128)  # Second feedforward layer\n",
    "\n",
    "        # Transformer encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=num_heads, dim_feedforward=transformer_hidden_size)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "        # Activation functions and dropout\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape should be (batch_size, input_size)\n",
    "        \n",
    "        # Reshape inputs if necessary, to ensure compatibility\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)  # Ensure it’s (batch_size, input_size)\n",
    "        \n",
    "        # Feed Forward NN layers\n",
    "        x = self.relu(self.fc1(x))  # Apply first FC layer\n",
    "        x = self.dropout(x)         # Apply dropout\n",
    "        x = self.relu(self.fc2(x))  # Apply second FC layer\n",
    "\n",
    "        # Add dimension for transformer (seq_len, batch_size, d_model)\n",
    "        x = x.unsqueeze(0)  # Add a sequence length dimension (seq_len=1)\n",
    "\n",
    "        # Transformer encoder expects shape (seq_len, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Remove sequence length dimension\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        # Final output layer\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x.view(-1, 1, 1)  # Adjust to expected output shape\n",
    "\n",
    "\n",
    "model = FNNTransformerHybrid(input_size=5, transformer_hidden_size=256, num_heads=4, transformer_layers=2)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.SmoothL1Loss()  # You can also use nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Defines the Long Short Term Memory class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True, \n",
    "                            dropout=dropout_rate)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # Output is 1 (for E')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should have shape (batch_size, num_features)\n",
    "        # Reshape x to (batch_size, seq_length, input_size)\n",
    "        x = x.view(x.size(0), 1, -1)  # 1 is the seq_length, -1 infers the input_size\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)  # Hidden state\n",
    "        c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)  # Cell state\n",
    "        \n",
    "        # Pass through LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]  # out: (batch_size, hidden_size)\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)  # out: (batch_size, 1)\n",
    "        \n",
    "        return out.view(-1, 1, 1)  # Adjust output shape\n",
    "    \n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]  # Number of input features (Stress, Strain, Freq, Time, Temp)\n",
    "hidden_size = 64  # Number of LSTM hidden units\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "dropout_rate = 0.2  # Dropout rate\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.SmoothL1Loss()  # You can use MSELoss if you prefer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\"\"\"\n",
    "\n",
    "# Define the hybrid architecture\n",
    "class CNN_FFN_Hybrid(nn.Module):\n",
    "    def __init__(self, input_size, cnn_output_size, hidden_size, num_classes):\n",
    "        super(CNN_FFN_Hybrid, self).__init__()\n",
    "        \n",
    "        # CNN Part\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected layers (Feed Forward Part)\n",
    "        self.fc1 = nn.Linear(cnn_output_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # CNN Part\n",
    "        x = x.unsqueeze(1)  # Adding channel dimension for 1D CNN\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        \n",
    "        # Flatten the output from CNN\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Feed Forward NN Part\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Hyperparameters and settings\n",
    "input_size = 5  # Adjust based on your input data\n",
    "cnn_output_size = 64 * (input_size // 4)  # Adjust based on CNN's output shape\n",
    "hidden_size = 128\n",
    "num_classes = 1  # Assuming regression task; for classification, adjust this\n",
    "\n",
    "model = CNN_FFN_Hybrid(input_size, cnn_output_size, hidden_size, num_classes)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Assuming regression task\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=1000):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions.append(outputs.numpy())\n",
    "            actuals.append(targets.numpy())\n",
    "    \n",
    "    return total_loss / len(test_loader), np.vstack(predictions), np.vstack(actuals)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, y_pred, y_true = evaluate(model, test_loader, criterion)\n",
    "print(f'Test Loss (MSE): {test_loss:.4f}')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Use this for CNN only\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            actuals.append(targets)\n",
    "            predictions.append(outputs)\n",
    "    \n",
    "    actuals = torch.cat(actuals).numpy().squeeze()  # Flatten to 1D\n",
    "    predictions = torch.cat(predictions).numpy().squeeze()  # Flatten to 1D\n",
    "    return actuals, predictions\n",
    "\n",
    "# Get the actual and predicted values\n",
    "actuals, predictions = evaluate_model(model, test_loader)\n",
    "\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(actuals, predictions, color='blue', label='Predicted vs Actual')\n",
    "plt.plot([min(actuals), max(actuals)], [min(actuals), max(actuals)], color='red', linestyle='--', label='Ideal Prediction')  # Line of perfect prediction\n",
    "plt.xlabel(\"Actual E' (MPa)\")\n",
    "plt.ylabel(\"Predicted E' (MPa)\")\n",
    "plt.title(\"Predicted vs Actual E' Values for Convolutional Neural Network with LSTM (Hybrid)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Actual vs Predicted\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_true, y_pred, alpha=0.7)\n",
    "plt.xlabel(\"Actual E' (MPa)\")\n",
    "plt.ylabel(\"Predicted E' (MPa)\")\n",
    "plt.title(\"Actual vs Predicted E' for Convolutional NN and Feed Forward NN Hybrid\")\n",
    "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')  # Diagonal line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
