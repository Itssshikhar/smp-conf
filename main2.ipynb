{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>F stat.</th>\n",
       "      <th>Strain stat</th>\n",
       "      <th>Stress stat</th>\n",
       "      <th>Lo</th>\n",
       "      <th>Lm</th>\n",
       "      <th>F dyn.</th>\n",
       "      <th>Strain dyn</th>\n",
       "      <th>Stress dyn</th>\n",
       "      <th>Time</th>\n",
       "      <th>f</th>\n",
       "      <th>T</th>\n",
       "      <th>E'</th>\n",
       "      <th>E''</th>\n",
       "      <th>|E*|</th>\n",
       "      <th>tan ä</th>\n",
       "      <th>F cont.</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>|J*|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>%</td>\n",
       "      <td>MPa</td>\n",
       "      <td>mm</td>\n",
       "      <td>mm</td>\n",
       "      <td>N</td>\n",
       "      <td>%</td>\n",
       "      <td>MPa</td>\n",
       "      <td>sec</td>\n",
       "      <td>Hz</td>\n",
       "      <td>°C</td>\n",
       "      <td>MPa</td>\n",
       "      <td>MPa</td>\n",
       "      <td>MPa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>°C</td>\n",
       "      <td>°C</td>\n",
       "      <td>1/MPa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.78481</td>\n",
       "      <td>0.301286</td>\n",
       "      <td>2.08792</td>\n",
       "      <td>2.26414</td>\n",
       "      <td>2.1838</td>\n",
       "      <td>0.778989</td>\n",
       "      <td>0.0470179</td>\n",
       "      <td>0.339922</td>\n",
       "      <td>9.68E+02</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.7</td>\n",
       "      <td>7.22E+02</td>\n",
       "      <td>33.92465</td>\n",
       "      <td>7.23E+02</td>\n",
       "      <td>0.046980</td>\n",
       "      <td>0.726986</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>1.38E-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.15294</td>\n",
       "      <td>0.167509</td>\n",
       "      <td>1.81219</td>\n",
       "      <td>2.24064</td>\n",
       "      <td>2.19597</td>\n",
       "      <td>0.78377</td>\n",
       "      <td>0.0470872</td>\n",
       "      <td>0.342009</td>\n",
       "      <td>1.40E+03</td>\n",
       "      <td>0.17609</td>\n",
       "      <td>25.8</td>\n",
       "      <td>7.26E+02</td>\n",
       "      <td>28.88208</td>\n",
       "      <td>7.26E+02</td>\n",
       "      <td>0.039800</td>\n",
       "      <td>0.822842</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>1.38E-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.15392</td>\n",
       "      <td>0.169736</td>\n",
       "      <td>1.81262</td>\n",
       "      <td>2.24177</td>\n",
       "      <td>2.1965</td>\n",
       "      <td>0.788822</td>\n",
       "      <td>0.0471716</td>\n",
       "      <td>0.344213</td>\n",
       "      <td>1.64E+03</td>\n",
       "      <td>0.31008</td>\n",
       "      <td>25.4</td>\n",
       "      <td>7.29E+02</td>\n",
       "      <td>28.14774</td>\n",
       "      <td>7.30E+02</td>\n",
       "      <td>0.038600</td>\n",
       "      <td>0.807583</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>1.37E-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.14512</td>\n",
       "      <td>0.171034</td>\n",
       "      <td>1.80878</td>\n",
       "      <td>2.24238</td>\n",
       "      <td>2.19677</td>\n",
       "      <td>0.78954</td>\n",
       "      <td>0.0470392</td>\n",
       "      <td>0.344527</td>\n",
       "      <td>1.79E+03</td>\n",
       "      <td>0.54601</td>\n",
       "      <td>25.1</td>\n",
       "      <td>7.32E+02</td>\n",
       "      <td>28.72457</td>\n",
       "      <td>7.32E+02</td>\n",
       "      <td>0.039250</td>\n",
       "      <td>0.817365</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>1.37E-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>152</td>\n",
       "      <td>1.04937</td>\n",
       "      <td>0.175751</td>\n",
       "      <td>0.457907</td>\n",
       "      <td>0.824722</td>\n",
       "      <td>0.777855</td>\n",
       "      <td>0.02027</td>\n",
       "      <td>0.046989</td>\n",
       "      <td>8.85E-03</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>5.24974</td>\n",
       "      <td>150</td>\n",
       "      <td>17.6731</td>\n",
       "      <td>6.48028</td>\n",
       "      <td>18.8237</td>\n",
       "      <td>0.366674</td>\n",
       "      <td>0.988731</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>0.0531244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>153</td>\n",
       "      <td>1.04742</td>\n",
       "      <td>0.173494</td>\n",
       "      <td>0.457056</td>\n",
       "      <td>0.825166</td>\n",
       "      <td>0.778901</td>\n",
       "      <td>0.0234622</td>\n",
       "      <td>0.04773</td>\n",
       "      <td>0.010238</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>9.24425</td>\n",
       "      <td>149.8</td>\n",
       "      <td>19.9398</td>\n",
       "      <td>7.9059</td>\n",
       "      <td>21.4499</td>\n",
       "      <td>0.396489</td>\n",
       "      <td>0.987753</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>0.0466202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>154</td>\n",
       "      <td>1.04644</td>\n",
       "      <td>0.17847</td>\n",
       "      <td>0.456628</td>\n",
       "      <td>0.826506</td>\n",
       "      <td>0.778914</td>\n",
       "      <td>0.0266739</td>\n",
       "      <td>0.0471866</td>\n",
       "      <td>0.0116395</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>16.2782</td>\n",
       "      <td>150.1</td>\n",
       "      <td>22.6029</td>\n",
       "      <td>9.87773</td>\n",
       "      <td>24.667</td>\n",
       "      <td>0.437012</td>\n",
       "      <td>0.979732</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>0.0405401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>155</td>\n",
       "      <td>1.05035</td>\n",
       "      <td>0.17802</td>\n",
       "      <td>0.458335</td>\n",
       "      <td>0.822343</td>\n",
       "      <td>0.774871</td>\n",
       "      <td>0.0313525</td>\n",
       "      <td>0.0472699</td>\n",
       "      <td>0.0136811</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>28.6642</td>\n",
       "      <td>150.2</td>\n",
       "      <td>26.2371</td>\n",
       "      <td>12.2181</td>\n",
       "      <td>28.9425</td>\n",
       "      <td>0.465682</td>\n",
       "      <td>0.987361</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>0.0345513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>156</td>\n",
       "      <td>1.0484</td>\n",
       "      <td>0.178665</td>\n",
       "      <td>0.457484</td>\n",
       "      <td>0.823507</td>\n",
       "      <td>0.775863</td>\n",
       "      <td>0.0365404</td>\n",
       "      <td>0.0461212</td>\n",
       "      <td>0.0159449</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>50</td>\n",
       "      <td>149.9</td>\n",
       "      <td>31.1211</td>\n",
       "      <td>15.0559</td>\n",
       "      <td>34.5717</td>\n",
       "      <td>0.483783</td>\n",
       "      <td>0.986383</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>0.0289254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      No  F stat. Strain stat Stress stat        Lo        Lm     F dyn.  \\\n",
       "0               N           %         MPa        mm        mm          N   \n",
       "1      1  4.78481    0.301286     2.08792   2.26414    2.1838   0.778989   \n",
       "2      2  4.15294    0.167509     1.81219   2.24064   2.19597    0.78377   \n",
       "3      3  4.15392    0.169736     1.81262   2.24177    2.1965   0.788822   \n",
       "4      4  4.14512    0.171034     1.80878   2.24238   2.19677    0.78954   \n",
       "..   ...      ...         ...         ...       ...       ...        ...   \n",
       "152  152  1.04937    0.175751    0.457907  0.824722  0.777855    0.02027   \n",
       "153  153  1.04742    0.173494    0.457056  0.825166  0.778901  0.0234622   \n",
       "154  154  1.04644     0.17847    0.456628  0.826506  0.778914  0.0266739   \n",
       "155  155  1.05035     0.17802    0.458335  0.822343  0.774871  0.0313525   \n",
       "156  156   1.0484    0.178665    0.457484  0.823507  0.775863  0.0365404   \n",
       "\n",
       "    Strain dyn Stress dyn      Time        f      T        E'       E''  \\\n",
       "0            %        MPa       sec       Hz     °C       MPa       MPa   \n",
       "1    0.0470179   0.339922  9.68E+02      0.1   26.7  7.22E+02  33.92465   \n",
       "2    0.0470872   0.342009  1.40E+03  0.17609   25.8  7.26E+02  28.88208   \n",
       "3    0.0471716   0.344213  1.64E+03  0.31008   25.4  7.29E+02  28.14774   \n",
       "4    0.0470392   0.344527  1.79E+03  0.54601   25.1  7.32E+02  28.72457   \n",
       "..         ...        ...       ...      ...    ...       ...       ...   \n",
       "152   0.046989   8.85E-03  2.52E+04  5.24974    150   17.6731   6.48028   \n",
       "153    0.04773   0.010238  2.52E+04  9.24425  149.8   19.9398    7.9059   \n",
       "154  0.0471866  0.0116395  2.52E+04  16.2782  150.1   22.6029   9.87773   \n",
       "155  0.0472699  0.0136811  2.52E+04  28.6642  150.2   26.2371   12.2181   \n",
       "156  0.0461212  0.0159449  2.52E+04       50  149.9   31.1211   15.0559   \n",
       "\n",
       "         |E*|     tan ä   F cont.       T2       T3       |J*|  \n",
       "0         MPa       NaN         N       °C       °C      1/MPa  \n",
       "1    7.23E+02  0.046980  0.726986    ---      ---     1.38E-03  \n",
       "2    7.26E+02  0.039800  0.822842    ---      ---     1.38E-03  \n",
       "3    7.30E+02  0.038600  0.807583    ---      ---     1.37E-03  \n",
       "4    7.32E+02  0.039250  0.817365    ---      ---     1.37E-03  \n",
       "..        ...       ...       ...      ...      ...        ...  \n",
       "152   18.8237  0.366674  0.988731    ---      ---    0.0531244  \n",
       "153   21.4499  0.396489  0.987753    ---      ---    0.0466202  \n",
       "154    24.667  0.437012  0.979732    ---      ---    0.0405401  \n",
       "155   28.9425  0.465682  0.987361    ---      ---    0.0345513  \n",
       "156   34.5717  0.483783  0.986383    ---      ---    0.0289254  \n",
       "\n",
       "[157 rows x 20 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('extensive_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>F stat.</th>\n",
       "      <th>Strain stat</th>\n",
       "      <th>Stress stat</th>\n",
       "      <th>Lo</th>\n",
       "      <th>Lm</th>\n",
       "      <th>F dyn.</th>\n",
       "      <th>Strain dyn</th>\n",
       "      <th>Stress dyn</th>\n",
       "      <th>Time</th>\n",
       "      <th>f</th>\n",
       "      <th>T</th>\n",
       "      <th>E'</th>\n",
       "      <th>E''</th>\n",
       "      <th>|E*|</th>\n",
       "      <th>tan ä</th>\n",
       "      <th>F cont.</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>|J*|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>%</td>\n",
       "      <td>MPa</td>\n",
       "      <td>mm</td>\n",
       "      <td>mm</td>\n",
       "      <td>N</td>\n",
       "      <td>%</td>\n",
       "      <td>MPa</td>\n",
       "      <td>sec</td>\n",
       "      <td>Hz</td>\n",
       "      <td>°C</td>\n",
       "      <td>MPa</td>\n",
       "      <td>MPa</td>\n",
       "      <td>MPa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>°C</td>\n",
       "      <td>°C</td>\n",
       "      <td>1/MPa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.78481</td>\n",
       "      <td>0.301286</td>\n",
       "      <td>2.08792</td>\n",
       "      <td>2.26414</td>\n",
       "      <td>2.1838</td>\n",
       "      <td>0.778989</td>\n",
       "      <td>0.0470179</td>\n",
       "      <td>0.339922</td>\n",
       "      <td>9.68E+02</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.7</td>\n",
       "      <td>7.22E+02</td>\n",
       "      <td>33.92465</td>\n",
       "      <td>7.23E+02</td>\n",
       "      <td>0.046980</td>\n",
       "      <td>0.726986</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>1.38E-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.15294</td>\n",
       "      <td>0.167509</td>\n",
       "      <td>1.81219</td>\n",
       "      <td>2.24064</td>\n",
       "      <td>2.19597</td>\n",
       "      <td>0.78377</td>\n",
       "      <td>0.0470872</td>\n",
       "      <td>0.342009</td>\n",
       "      <td>1.40E+03</td>\n",
       "      <td>0.17609</td>\n",
       "      <td>25.8</td>\n",
       "      <td>7.26E+02</td>\n",
       "      <td>28.88208</td>\n",
       "      <td>7.26E+02</td>\n",
       "      <td>0.039800</td>\n",
       "      <td>0.822842</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>1.38E-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.15392</td>\n",
       "      <td>0.169736</td>\n",
       "      <td>1.81262</td>\n",
       "      <td>2.24177</td>\n",
       "      <td>2.1965</td>\n",
       "      <td>0.788822</td>\n",
       "      <td>0.0471716</td>\n",
       "      <td>0.344213</td>\n",
       "      <td>1.64E+03</td>\n",
       "      <td>0.31008</td>\n",
       "      <td>25.4</td>\n",
       "      <td>7.29E+02</td>\n",
       "      <td>28.14774</td>\n",
       "      <td>7.30E+02</td>\n",
       "      <td>0.038600</td>\n",
       "      <td>0.807583</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>1.37E-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.14512</td>\n",
       "      <td>0.171034</td>\n",
       "      <td>1.80878</td>\n",
       "      <td>2.24238</td>\n",
       "      <td>2.19677</td>\n",
       "      <td>0.78954</td>\n",
       "      <td>0.0470392</td>\n",
       "      <td>0.344527</td>\n",
       "      <td>1.79E+03</td>\n",
       "      <td>0.54601</td>\n",
       "      <td>25.1</td>\n",
       "      <td>7.32E+02</td>\n",
       "      <td>28.72457</td>\n",
       "      <td>7.32E+02</td>\n",
       "      <td>0.039250</td>\n",
       "      <td>0.817365</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>1.37E-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>152</td>\n",
       "      <td>1.04937</td>\n",
       "      <td>0.175751</td>\n",
       "      <td>0.457907</td>\n",
       "      <td>0.824722</td>\n",
       "      <td>0.777855</td>\n",
       "      <td>0.02027</td>\n",
       "      <td>0.046989</td>\n",
       "      <td>8.85E-03</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>5.24974</td>\n",
       "      <td>150</td>\n",
       "      <td>17.6731</td>\n",
       "      <td>6.48028</td>\n",
       "      <td>18.8237</td>\n",
       "      <td>0.366674</td>\n",
       "      <td>0.988731</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>0.0531244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>153</td>\n",
       "      <td>1.04742</td>\n",
       "      <td>0.173494</td>\n",
       "      <td>0.457056</td>\n",
       "      <td>0.825166</td>\n",
       "      <td>0.778901</td>\n",
       "      <td>0.0234622</td>\n",
       "      <td>0.04773</td>\n",
       "      <td>0.010238</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>9.24425</td>\n",
       "      <td>149.8</td>\n",
       "      <td>19.9398</td>\n",
       "      <td>7.9059</td>\n",
       "      <td>21.4499</td>\n",
       "      <td>0.396489</td>\n",
       "      <td>0.987753</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>0.0466202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>154</td>\n",
       "      <td>1.04644</td>\n",
       "      <td>0.17847</td>\n",
       "      <td>0.456628</td>\n",
       "      <td>0.826506</td>\n",
       "      <td>0.778914</td>\n",
       "      <td>0.0266739</td>\n",
       "      <td>0.0471866</td>\n",
       "      <td>0.0116395</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>16.2782</td>\n",
       "      <td>150.1</td>\n",
       "      <td>22.6029</td>\n",
       "      <td>9.87773</td>\n",
       "      <td>24.667</td>\n",
       "      <td>0.437012</td>\n",
       "      <td>0.979732</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>0.0405401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>155</td>\n",
       "      <td>1.05035</td>\n",
       "      <td>0.17802</td>\n",
       "      <td>0.458335</td>\n",
       "      <td>0.822343</td>\n",
       "      <td>0.774871</td>\n",
       "      <td>0.0313525</td>\n",
       "      <td>0.0472699</td>\n",
       "      <td>0.0136811</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>28.6642</td>\n",
       "      <td>150.2</td>\n",
       "      <td>26.2371</td>\n",
       "      <td>12.2181</td>\n",
       "      <td>28.9425</td>\n",
       "      <td>0.465682</td>\n",
       "      <td>0.987361</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>0.0345513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>156</td>\n",
       "      <td>1.0484</td>\n",
       "      <td>0.178665</td>\n",
       "      <td>0.457484</td>\n",
       "      <td>0.823507</td>\n",
       "      <td>0.775863</td>\n",
       "      <td>0.0365404</td>\n",
       "      <td>0.0461212</td>\n",
       "      <td>0.0159449</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>50</td>\n",
       "      <td>149.9</td>\n",
       "      <td>31.1211</td>\n",
       "      <td>15.0559</td>\n",
       "      <td>34.5717</td>\n",
       "      <td>0.483783</td>\n",
       "      <td>0.986383</td>\n",
       "      <td>---</td>\n",
       "      <td>---</td>\n",
       "      <td>0.0289254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      No  F stat. Strain stat Stress stat        Lo        Lm     F dyn.  \\\n",
       "0               N           %         MPa        mm        mm          N   \n",
       "1      1  4.78481    0.301286     2.08792   2.26414    2.1838   0.778989   \n",
       "2      2  4.15294    0.167509     1.81219   2.24064   2.19597    0.78377   \n",
       "3      3  4.15392    0.169736     1.81262   2.24177    2.1965   0.788822   \n",
       "4      4  4.14512    0.171034     1.80878   2.24238   2.19677    0.78954   \n",
       "..   ...      ...         ...         ...       ...       ...        ...   \n",
       "152  152  1.04937    0.175751    0.457907  0.824722  0.777855    0.02027   \n",
       "153  153  1.04742    0.173494    0.457056  0.825166  0.778901  0.0234622   \n",
       "154  154  1.04644     0.17847    0.456628  0.826506  0.778914  0.0266739   \n",
       "155  155  1.05035     0.17802    0.458335  0.822343  0.774871  0.0313525   \n",
       "156  156   1.0484    0.178665    0.457484  0.823507  0.775863  0.0365404   \n",
       "\n",
       "    Strain dyn Stress dyn      Time        f      T        E'       E''  \\\n",
       "0            %        MPa       sec       Hz     °C       MPa       MPa   \n",
       "1    0.0470179   0.339922  9.68E+02      0.1   26.7  7.22E+02  33.92465   \n",
       "2    0.0470872   0.342009  1.40E+03  0.17609   25.8  7.26E+02  28.88208   \n",
       "3    0.0471716   0.344213  1.64E+03  0.31008   25.4  7.29E+02  28.14774   \n",
       "4    0.0470392   0.344527  1.79E+03  0.54601   25.1  7.32E+02  28.72457   \n",
       "..         ...        ...       ...      ...    ...       ...       ...   \n",
       "152   0.046989   8.85E-03  2.52E+04  5.24974    150   17.6731   6.48028   \n",
       "153    0.04773   0.010238  2.52E+04  9.24425  149.8   19.9398    7.9059   \n",
       "154  0.0471866  0.0116395  2.52E+04  16.2782  150.1   22.6029   9.87773   \n",
       "155  0.0472699  0.0136811  2.52E+04  28.6642  150.2   26.2371   12.2181   \n",
       "156  0.0461212  0.0159449  2.52E+04       50  149.9   31.1211   15.0559   \n",
       "\n",
       "         |E*|     tan ä   F cont.       T2       T3       |J*|  \n",
       "0         MPa       NaN         N       °C       °C      1/MPa  \n",
       "1    7.23E+02  0.046980  0.726986    ---      ---     1.38E-03  \n",
       "2    7.26E+02  0.039800  0.822842    ---      ---     1.38E-03  \n",
       "3    7.30E+02  0.038600  0.807583    ---      ---     1.37E-03  \n",
       "4    7.32E+02  0.039250  0.817365    ---      ---     1.37E-03  \n",
       "..        ...       ...       ...      ...      ...        ...  \n",
       "152   18.8237  0.366674  0.988731    ---      ---    0.0531244  \n",
       "153   21.4499  0.396489  0.987753    ---      ---    0.0466202  \n",
       "154    24.667  0.437012  0.979732    ---      ---    0.0405401  \n",
       "155   28.9425  0.465682  0.987361    ---      ---    0.0345513  \n",
       "156   34.5717  0.483783  0.986383    ---      ---    0.0289254  \n",
       "\n",
       "[157 rows x 20 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data = pd.read_csv('extensive_dataset.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strain dyn</th>\n",
       "      <th>Stress dyn</th>\n",
       "      <th>Time</th>\n",
       "      <th>f</th>\n",
       "      <th>T</th>\n",
       "      <th>E'</th>\n",
       "      <th>E''</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>%</td>\n",
       "      <td>MPa</td>\n",
       "      <td>sec</td>\n",
       "      <td>Hz</td>\n",
       "      <td>°C</td>\n",
       "      <td>MPa</td>\n",
       "      <td>MPa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0470179</td>\n",
       "      <td>0.339922</td>\n",
       "      <td>9.68E+02</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.7</td>\n",
       "      <td>7.22E+02</td>\n",
       "      <td>33.92465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0470872</td>\n",
       "      <td>0.342009</td>\n",
       "      <td>1.40E+03</td>\n",
       "      <td>0.17609</td>\n",
       "      <td>25.8</td>\n",
       "      <td>7.26E+02</td>\n",
       "      <td>28.88208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0471716</td>\n",
       "      <td>0.344213</td>\n",
       "      <td>1.64E+03</td>\n",
       "      <td>0.31008</td>\n",
       "      <td>25.4</td>\n",
       "      <td>7.29E+02</td>\n",
       "      <td>28.14774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0470392</td>\n",
       "      <td>0.344527</td>\n",
       "      <td>1.79E+03</td>\n",
       "      <td>0.54601</td>\n",
       "      <td>25.1</td>\n",
       "      <td>7.32E+02</td>\n",
       "      <td>28.72457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.046989</td>\n",
       "      <td>8.85E-03</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>5.24974</td>\n",
       "      <td>150</td>\n",
       "      <td>17.6731</td>\n",
       "      <td>6.48028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.04773</td>\n",
       "      <td>0.010238</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>9.24425</td>\n",
       "      <td>149.8</td>\n",
       "      <td>19.9398</td>\n",
       "      <td>7.9059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.0471866</td>\n",
       "      <td>0.0116395</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>16.2782</td>\n",
       "      <td>150.1</td>\n",
       "      <td>22.6029</td>\n",
       "      <td>9.87773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.0472699</td>\n",
       "      <td>0.0136811</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>28.6642</td>\n",
       "      <td>150.2</td>\n",
       "      <td>26.2371</td>\n",
       "      <td>12.2181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.0461212</td>\n",
       "      <td>0.0159449</td>\n",
       "      <td>2.52E+04</td>\n",
       "      <td>50</td>\n",
       "      <td>149.9</td>\n",
       "      <td>31.1211</td>\n",
       "      <td>15.0559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Strain dyn Stress dyn      Time        f      T        E'       E''\n",
       "0            %        MPa       sec       Hz     °C       MPa       MPa\n",
       "1    0.0470179   0.339922  9.68E+02      0.1   26.7  7.22E+02  33.92465\n",
       "2    0.0470872   0.342009  1.40E+03  0.17609   25.8  7.26E+02  28.88208\n",
       "3    0.0471716   0.344213  1.64E+03  0.31008   25.4  7.29E+02  28.14774\n",
       "4    0.0470392   0.344527  1.79E+03  0.54601   25.1  7.32E+02  28.72457\n",
       "..         ...        ...       ...      ...    ...       ...       ...\n",
       "152   0.046989   8.85E-03  2.52E+04  5.24974    150   17.6731   6.48028\n",
       "153    0.04773   0.010238  2.52E+04  9.24425  149.8   19.9398    7.9059\n",
       "154  0.0471866  0.0116395  2.52E+04  16.2782  150.1   22.6029   9.87773\n",
       "155  0.0472699  0.0136811  2.52E+04  28.6642  150.2   26.2371   12.2181\n",
       "156  0.0461212  0.0159449  2.52E+04       50  149.9   31.1211   15.0559\n",
       "\n",
       "[157 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data = data.iloc[:, 7:14]\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.columns = ['Strain Dynamic (%)', 'Stress Dynamic (MPa)', 'Time (sec)', 'freq (Hz)', 'Temp (C)', \"E' (MPa)\", \"E'' (MPa)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['Strain Dynamic (%)'] = pd.to_numeric(cleaned_data['Strain Dynamic (%)'], errors='coerce')\n",
    "cleaned_data['Stress Dynamic (MPa)'] = pd.to_numeric(cleaned_data['Stress Dynamic (MPa)'], errors='coerce')\n",
    "cleaned_data[\"Time (sec)\"] = pd.to_numeric(cleaned_data[\"Time (sec)\"], errors='coerce')\n",
    "cleaned_data[\"freq (Hz)\"] = pd.to_numeric(cleaned_data[\"freq (Hz)\"], errors='coerce')\n",
    "cleaned_data['Temp (C)'] = pd.to_numeric(cleaned_data['Temp (C)'], errors='coerce')\n",
    "cleaned_data[\"E' (MPa)\"] = pd.to_numeric(cleaned_data[\"E' (MPa)\"], errors='coerce')\n",
    "cleaned_data[\"E'' (MPa)\"] = pd.to_numeric(cleaned_data[\"E'' (MPa)\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strain Dynamic (%)</th>\n",
       "      <th>Stress Dynamic (MPa)</th>\n",
       "      <th>Time (sec)</th>\n",
       "      <th>freq (Hz)</th>\n",
       "      <th>Temp (C)</th>\n",
       "      <th>E' (MPa)</th>\n",
       "      <th>E'' (MPa)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047018</td>\n",
       "      <td>0.339922</td>\n",
       "      <td>968.0</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>26.7</td>\n",
       "      <td>722.0000</td>\n",
       "      <td>33.92465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.047087</td>\n",
       "      <td>0.342009</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>0.17609</td>\n",
       "      <td>25.8</td>\n",
       "      <td>726.0000</td>\n",
       "      <td>28.88208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047172</td>\n",
       "      <td>0.344213</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>0.31008</td>\n",
       "      <td>25.4</td>\n",
       "      <td>729.0000</td>\n",
       "      <td>28.14774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.047039</td>\n",
       "      <td>0.344527</td>\n",
       "      <td>1790.0</td>\n",
       "      <td>0.54601</td>\n",
       "      <td>25.1</td>\n",
       "      <td>732.0000</td>\n",
       "      <td>28.72457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.047088</td>\n",
       "      <td>0.346514</td>\n",
       "      <td>1880.0</td>\n",
       "      <td>0.96147</td>\n",
       "      <td>25.0</td>\n",
       "      <td>735.0000</td>\n",
       "      <td>30.20620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.046989</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>25200.0</td>\n",
       "      <td>5.24974</td>\n",
       "      <td>150.0</td>\n",
       "      <td>17.6731</td>\n",
       "      <td>6.48028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.047730</td>\n",
       "      <td>0.010238</td>\n",
       "      <td>25200.0</td>\n",
       "      <td>9.24425</td>\n",
       "      <td>149.8</td>\n",
       "      <td>19.9398</td>\n",
       "      <td>7.90590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.047187</td>\n",
       "      <td>0.011640</td>\n",
       "      <td>25200.0</td>\n",
       "      <td>16.27820</td>\n",
       "      <td>150.1</td>\n",
       "      <td>22.6029</td>\n",
       "      <td>9.87773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.047270</td>\n",
       "      <td>0.013681</td>\n",
       "      <td>25200.0</td>\n",
       "      <td>28.66420</td>\n",
       "      <td>150.2</td>\n",
       "      <td>26.2371</td>\n",
       "      <td>12.21810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.046121</td>\n",
       "      <td>0.015945</td>\n",
       "      <td>25200.0</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>149.9</td>\n",
       "      <td>31.1211</td>\n",
       "      <td>15.05590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Strain Dynamic (%)  Stress Dynamic (MPa)  Time (sec)  freq (Hz)  \\\n",
       "1              0.047018              0.339922       968.0    0.10000   \n",
       "2              0.047087              0.342009      1400.0    0.17609   \n",
       "3              0.047172              0.344213      1640.0    0.31008   \n",
       "4              0.047039              0.344527      1790.0    0.54601   \n",
       "5              0.047088              0.346514      1880.0    0.96147   \n",
       "..                  ...                   ...         ...        ...   \n",
       "152            0.046989              0.008850     25200.0    5.24974   \n",
       "153            0.047730              0.010238     25200.0    9.24425   \n",
       "154            0.047187              0.011640     25200.0   16.27820   \n",
       "155            0.047270              0.013681     25200.0   28.66420   \n",
       "156            0.046121              0.015945     25200.0   50.00000   \n",
       "\n",
       "     Temp (C)  E' (MPa)  E'' (MPa)  \n",
       "1        26.7  722.0000   33.92465  \n",
       "2        25.8  726.0000   28.88208  \n",
       "3        25.4  729.0000   28.14774  \n",
       "4        25.1  732.0000   28.72457  \n",
       "5        25.0  735.0000   30.20620  \n",
       "..        ...       ...        ...  \n",
       "152     150.0   17.6731    6.48028  \n",
       "153     149.8   19.9398    7.90590  \n",
       "154     150.1   22.6029    9.87773  \n",
       "155     150.2   26.2371   12.21810  \n",
       "156     149.9   31.1211   15.05590  \n",
       "\n",
       "[156 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.dropna(how='all', inplace=True)\n",
    "cleaned_data.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strain Dynamic (%)</th>\n",
       "      <th>Stress Dynamic (MPa)</th>\n",
       "      <th>Time (sec)</th>\n",
       "      <th>freq (Hz)</th>\n",
       "      <th>Temp (C)</th>\n",
       "      <th>E' (MPa)</th>\n",
       "      <th>E'' (MPa)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047018</td>\n",
       "      <td>0.339922</td>\n",
       "      <td>968.0</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>26.7</td>\n",
       "      <td>722.0000</td>\n",
       "      <td>33.92465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.047087</td>\n",
       "      <td>0.342009</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>0.17609</td>\n",
       "      <td>25.8</td>\n",
       "      <td>726.0000</td>\n",
       "      <td>28.88208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047172</td>\n",
       "      <td>0.344213</td>\n",
       "      <td>1640.0</td>\n",
       "      <td>0.31008</td>\n",
       "      <td>25.4</td>\n",
       "      <td>729.0000</td>\n",
       "      <td>28.14774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.047039</td>\n",
       "      <td>0.344527</td>\n",
       "      <td>1790.0</td>\n",
       "      <td>0.54601</td>\n",
       "      <td>25.1</td>\n",
       "      <td>732.0000</td>\n",
       "      <td>28.72457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.047088</td>\n",
       "      <td>0.346514</td>\n",
       "      <td>1880.0</td>\n",
       "      <td>0.96147</td>\n",
       "      <td>25.0</td>\n",
       "      <td>735.0000</td>\n",
       "      <td>30.20620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.046989</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>25200.0</td>\n",
       "      <td>5.24974</td>\n",
       "      <td>150.0</td>\n",
       "      <td>17.6731</td>\n",
       "      <td>6.48028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.047730</td>\n",
       "      <td>0.010238</td>\n",
       "      <td>25200.0</td>\n",
       "      <td>9.24425</td>\n",
       "      <td>149.8</td>\n",
       "      <td>19.9398</td>\n",
       "      <td>7.90590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.047187</td>\n",
       "      <td>0.011640</td>\n",
       "      <td>25200.0</td>\n",
       "      <td>16.27820</td>\n",
       "      <td>150.1</td>\n",
       "      <td>22.6029</td>\n",
       "      <td>9.87773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.047270</td>\n",
       "      <td>0.013681</td>\n",
       "      <td>25200.0</td>\n",
       "      <td>28.66420</td>\n",
       "      <td>150.2</td>\n",
       "      <td>26.2371</td>\n",
       "      <td>12.21810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.046121</td>\n",
       "      <td>0.015945</td>\n",
       "      <td>25200.0</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>149.9</td>\n",
       "      <td>31.1211</td>\n",
       "      <td>15.05590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Strain Dynamic (%)  Stress Dynamic (MPa)  Time (sec)  freq (Hz)  \\\n",
       "1              0.047018              0.339922       968.0    0.10000   \n",
       "2              0.047087              0.342009      1400.0    0.17609   \n",
       "3              0.047172              0.344213      1640.0    0.31008   \n",
       "4              0.047039              0.344527      1790.0    0.54601   \n",
       "5              0.047088              0.346514      1880.0    0.96147   \n",
       "..                  ...                   ...         ...        ...   \n",
       "152            0.046989              0.008850     25200.0    5.24974   \n",
       "153            0.047730              0.010238     25200.0    9.24425   \n",
       "154            0.047187              0.011640     25200.0   16.27820   \n",
       "155            0.047270              0.013681     25200.0   28.66420   \n",
       "156            0.046121              0.015945     25200.0   50.00000   \n",
       "\n",
       "     Temp (C)  E' (MPa)  E'' (MPa)  \n",
       "1        26.7  722.0000   33.92465  \n",
       "2        25.8  726.0000   28.88208  \n",
       "3        25.4  729.0000   28.14774  \n",
       "4        25.1  732.0000   28.72457  \n",
       "5        25.0  735.0000   30.20620  \n",
       "..        ...       ...        ...  \n",
       "152     150.0   17.6731    6.48028  \n",
       "153     149.8   19.9398    7.90590  \n",
       "154     150.1   22.6029    9.87773  \n",
       "155     150.2   26.2371   12.21810  \n",
       "156     149.9   31.1211   15.05590  \n",
       "\n",
       "[156 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned_data[['Strain Dynamic (%)', 'Stress Dynamic (MPa)', 'Time (sec)', 'freq (Hz)', 'Temp (C)']].values\n",
    "y = cleaned_data[[\"E' (MPa)\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Strain Dynamic (%)      0.139188\n",
       "Stress Dynamic (MPa)    0.999896\n",
       "Time (sec)             -0.968622\n",
       "freq (Hz)               0.079635\n",
       "Temp (C)               -0.974389\n",
       "E' (MPa)                1.000000\n",
       "E'' (MPa)               0.406209\n",
       "Name: E' (MPa), dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the correlation of column A against all others\n",
    "corr_matrix = cleaned_data.corr()[\"E' (MPa)\"]\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[748.    ],\n",
       "       [ 19.9398],\n",
       "       [115.766 ],\n",
       "       [647.    ],\n",
       "       [686.    ],\n",
       "       [721.    ],\n",
       "       [701.    ],\n",
       "       [620.    ],\n",
       "       [725.    ],\n",
       "       [437.489 ],\n",
       "       [710.    ],\n",
       "       [129.961 ],\n",
       "       [479.038 ],\n",
       "       [708.    ],\n",
       "       [374.861 ],\n",
       "       [595.    ],\n",
       "       [ 33.8348],\n",
       "       [657.    ],\n",
       "       [716.    ],\n",
       "       [103.171 ],\n",
       "       [ 90.9952],\n",
       "       [660.    ],\n",
       "       [586.    ],\n",
       "       [435.38  ],\n",
       "       [330.043 ],\n",
       "       [293.799 ],\n",
       "       [694.    ],\n",
       "       [ 29.5145],\n",
       "       [543.    ],\n",
       "       [ 68.9214],\n",
       "       [503.    ],\n",
       "       [361.169 ],\n",
       "       [ 79.0982],\n",
       "       [235.004 ],\n",
       "       [691.    ],\n",
       "       [ 76.2148],\n",
       "       [590.    ],\n",
       "       [ 12.3904],\n",
       "       [465.901 ],\n",
       "       [646.    ],\n",
       "       [148.878 ],\n",
       "       [ 12.7013],\n",
       "       [684.    ],\n",
       "       [739.    ],\n",
       "       [534.    ],\n",
       "       [566.    ],\n",
       "       [741.    ],\n",
       "       [727.    ],\n",
       "       [ 26.2371],\n",
       "       [726.    ],\n",
       "       [ 15.6982],\n",
       "       [411.765 ],\n",
       "       [729.    ],\n",
       "       [ 13.1736],\n",
       "       [611.    ],\n",
       "       [511.    ],\n",
       "       [ 22.9624],\n",
       "       [296.852 ],\n",
       "       [ 14.8018],\n",
       "       [561.    ],\n",
       "       [210.024 ],\n",
       "       [744.    ],\n",
       "       [277.594 ],\n",
       "       [652.    ],\n",
       "       [ 17.6731],\n",
       "       [523.    ],\n",
       "       [ 59.4681],\n",
       "       [ 60.7615],\n",
       "       [688.    ],\n",
       "       [228.156 ],\n",
       "       [661.    ],\n",
       "       [655.    ],\n",
       "       [497.374 ],\n",
       "       [ 51.1455],\n",
       "       [722.    ],\n",
       "       [693.    ],\n",
       "       [195.529 ],\n",
       "       [ 39.0036],\n",
       "       [ 17.6256],\n",
       "       [257.695 ],\n",
       "       [723.    ],\n",
       "       [144.047 ],\n",
       "       [464.084 ],\n",
       "       [732.    ],\n",
       "       [160.917 ],\n",
       "       [511.    ],\n",
       "       [507.    ],\n",
       "       [ 29.2329],\n",
       "       [ 12.1864],\n",
       "       [424.891 ],\n",
       "       [718.    ],\n",
       "       [663.    ],\n",
       "       [580.    ],\n",
       "       [240.19  ],\n",
       "       [481.482 ],\n",
       "       [396.268 ],\n",
       "       [711.    ],\n",
       "       [110.966 ],\n",
       "       [429.353 ],\n",
       "       [566.    ],\n",
       "       [682.    ],\n",
       "       [663.    ],\n",
       "       [ 70.5251],\n",
       "       [ 22.6029],\n",
       "       [ 36.3562],\n",
       "       [649.    ],\n",
       "       [729.    ],\n",
       "       [702.    ],\n",
       "       [491.745 ],\n",
       "       [732.    ],\n",
       "       [600.    ],\n",
       "       [405.73  ],\n",
       "       [342.569 ],\n",
       "       [ 44.913 ],\n",
       "       [339.492 ],\n",
       "       [114.649 ],\n",
       "       [646.    ],\n",
       "       [751.    ],\n",
       "       [723.    ],\n",
       "       [579.    ],\n",
       "       [ 14.0382],\n",
       "       [379.898 ],\n",
       "       [350.586 ],\n",
       "       [605.    ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).unsqueeze(2) # unsqueeze for CNN, do not need for FFNN\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).unsqueeze(2) # unsqueeze for CNN, do not need for FFNN\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Normal CNN just working with some extra functions\n",
    "\n",
    "# Define the deeper CNN model with BatchNorm, Dropout, and Xavier Initialization\n",
    "class DeeperCNNWithBNDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeeperCNNWithBNDropout, self).__init__()\n",
    "\n",
    "        # First block of convolutions\n",
    "        self.conv1 = nn.Conv1d(in_channels=5, out_channels=32, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        # Second block of convolutions\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.conv4 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=1)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=1)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        # Fully connected layers (fc1 input size will be set dynamically)\n",
    "        self.fc1 = None  # Placeholder, will be set dynamically in forward\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "        # Apply Xavier initialization\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Second block\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten to [batch_size, features]\n",
    "\n",
    "        # Dynamically define fc1 based on the flattened size of x\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(x.size(1), 256).to(x.device)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # Initialize the weights of the model using Xavier initialization\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "# Instantiate the model\n",
    "model = DeeperCNNWithBNDropout()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Normal Feed Forward Neural Network that works with SmoothL1Loss instead of MSE\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "       # Input layer (taking 5 inputs: Stress, Strain, Freq, Time, Temp)\n",
    "        self.fc1 = nn.Linear(5, 64)  # 5 inputs, 64 neurons in the first layer\n",
    "        self.fc2 = nn.Linear(64, 128)  # 64 neurons in the second layer, 128 in the next\n",
    "        self.fc3 = nn.Linear(128, 1)  # 128 neurons to 1 output (E')\n",
    "\n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure x is flattened to shape [batch_size, input_size]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))  # Apply first layer and ReLU activation\n",
    "        x = self.dropout(x)  # Dropout\n",
    "        x = self.relu(self.fc2(x))  # Apply second layer and ReLU activation\n",
    "        x = self.dropout(x)  # Dropout\n",
    "        x = self.fc3(x)  # Output layer (no activation for regression)\n",
    "        return x.view(-1, 1, 1)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "print(f\"This is the shape of Input size: {input_size}\")\n",
    "model = FeedForwardNN(input_size)\n",
    "\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Define the CNN + LSTM Hybrid Model\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=5, out_channels=32, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout_cnn = nn.Dropout(0.3)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=64, num_layers=1, batch_first=True)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN layers\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout_cnn(x)\n",
    "\n",
    "        # Prepare the input for LSTM by permuting to [batch_size, sequence_length, features]\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # LSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        # Take the last output of the LSTM\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x.view(-1, 1)\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNNLSTM()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Defining the FNN(Feed Forward Neural Network) + Transformer\n",
    "class FNNTransformerHybrid(nn.Module):\n",
    "    def __init__(self, input_size, transformer_hidden_size, num_heads, transformer_layers):\n",
    "        super(FNNTransformerHybrid, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # First feedforward layer\n",
    "        self.fc2 = nn.Linear(64, 128)  # Second feedforward layer\n",
    "\n",
    "        # Transformer encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=num_heads, dim_feedforward=transformer_hidden_size)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "        # Activation functions and dropout\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape should be (batch_size, input_size)\n",
    "        \n",
    "        # Reshape inputs if necessary, to ensure compatibility\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)  # Ensure it’s (batch_size, input_size)\n",
    "        \n",
    "        # Feed Forward NN layers\n",
    "        x = self.relu(self.fc1(x))  # Apply first FC layer\n",
    "        x = self.dropout(x)         # Apply dropout\n",
    "        x = self.relu(self.fc2(x))  # Apply second FC layer\n",
    "\n",
    "        # Add dimension for transformer (seq_len, batch_size, d_model)\n",
    "        x = x.unsqueeze(0)  # Add a sequence length dimension (seq_len=1)\n",
    "\n",
    "        # Transformer encoder expects shape (seq_len, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Remove sequence length dimension\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        # Final output layer\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x.view(-1, 1, 1)  # Adjust to expected output shape\n",
    "\n",
    "\n",
    "model = FNNTransformerHybrid(input_size=5, transformer_hidden_size=256, num_heads=4, transformer_layers=2)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.SmoothL1Loss()  # You can also use nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Defines the Long Short Term Memory class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True, \n",
    "                            dropout=dropout_rate)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # Output is 1 (for E')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should have shape (batch_size, num_features)\n",
    "        # Reshape x to (batch_size, seq_length, input_size)\n",
    "        x = x.view(x.size(0), 1, -1)  # 1 is the seq_length, -1 infers the input_size\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)  # Hidden state\n",
    "        c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)  # Cell state\n",
    "        \n",
    "        # Pass through LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]  # out: (batch_size, hidden_size)\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)  # out: (batch_size, 1)\n",
    "        \n",
    "        return out.view(-1, 1, 1)  # Adjust output shape\n",
    "    \n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]  # Number of input features (Stress, Strain, Freq, Time, Temp)\n",
    "hidden_size = 64  # Number of LSTM hidden units\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "dropout_rate = 0.2  # Dropout rate\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.SmoothL1Loss()  # You can use MSELoss if you prefer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\"\"\"\n",
    "\n",
    "# Define Bidirectional LSTM network\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_rate, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)  # Multiply hidden_size by 2 due to bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input to (batch_size, seq_length, input_size)\n",
    "        x = x.view(x.size(0), 1, -1)  # 1 is the sequence length\n",
    "        h0 = torch.zeros(num_layers * 2, x.size(0), hidden_size).to(x.device)  # Multiply num_layers by 2 for bidirection\n",
    "        c0 = torch.zeros(num_layers * 2, x.size(0), hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.bilstm(x, (h0, c0))  # Pass through Bidirectional LSTM\n",
    "        out = out[:, -1, :]  # Get the last output\n",
    "        out = self.fc(out)  # Fully connected layer\n",
    "        return out.view(-1, 1, 1)  # Reshape to (batch_size, 1, 1)\n",
    "    \n",
    "# Model hyperparameters\n",
    "input_size = X_train.shape[1]  # Number of input features (e.g., 5)\n",
    "hidden_size = 128  # Number of hidden units in the LSTM\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "dropout_rate = 0.2  # Dropout rate\n",
    "\n",
    "# Initialize the BiLSTM model\n",
    "model = BiLSTMModel(input_size, hidden_size, num_layers, dropout_rate)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 416.21924209594727\n",
      "Epoch 2/1000, Loss: 419.80323028564453\n",
      "Epoch 3/1000, Loss: 417.25837326049805\n",
      "Epoch 4/1000, Loss: 422.8191909790039\n",
      "Epoch 5/1000, Loss: 418.8107223510742\n",
      "Epoch 6/1000, Loss: 421.28461837768555\n",
      "Epoch 7/1000, Loss: 417.63029861450195\n",
      "Epoch 8/1000, Loss: 420.61257553100586\n",
      "Epoch 9/1000, Loss: 422.80955505371094\n",
      "Epoch 10/1000, Loss: 417.6693534851074\n",
      "Epoch 11/1000, Loss: 415.02885818481445\n",
      "Epoch 12/1000, Loss: 415.0501136779785\n",
      "Epoch 13/1000, Loss: 413.11645889282227\n",
      "Epoch 14/1000, Loss: 419.71865463256836\n",
      "Epoch 15/1000, Loss: 422.915584564209\n",
      "Epoch 16/1000, Loss: 417.22801208496094\n",
      "Epoch 17/1000, Loss: 419.2961730957031\n",
      "Epoch 18/1000, Loss: 414.8710289001465\n",
      "Epoch 19/1000, Loss: 417.6537895202637\n",
      "Epoch 20/1000, Loss: 417.98483657836914\n",
      "Epoch 21/1000, Loss: 416.79387283325195\n",
      "Epoch 22/1000, Loss: 417.297176361084\n",
      "Epoch 23/1000, Loss: 413.8353805541992\n",
      "Epoch 24/1000, Loss: 411.2418212890625\n",
      "Epoch 25/1000, Loss: 420.5347671508789\n",
      "Epoch 26/1000, Loss: 417.43566513061523\n",
      "Epoch 27/1000, Loss: 415.2590103149414\n",
      "Epoch 28/1000, Loss: 417.9608612060547\n",
      "Epoch 29/1000, Loss: 416.39109802246094\n",
      "Epoch 30/1000, Loss: 416.5643310546875\n",
      "Epoch 31/1000, Loss: 418.71662521362305\n",
      "Epoch 32/1000, Loss: 417.1229438781738\n",
      "Epoch 33/1000, Loss: 420.8130683898926\n",
      "Epoch 34/1000, Loss: 419.4069519042969\n",
      "Epoch 35/1000, Loss: 415.1688117980957\n",
      "Epoch 36/1000, Loss: 416.23883056640625\n",
      "Epoch 37/1000, Loss: 419.4063186645508\n",
      "Epoch 38/1000, Loss: 414.0232048034668\n",
      "Epoch 39/1000, Loss: 409.1965084075928\n",
      "Epoch 40/1000, Loss: 415.58174896240234\n",
      "Epoch 41/1000, Loss: 418.9679412841797\n",
      "Epoch 42/1000, Loss: 418.55675888061523\n",
      "Epoch 43/1000, Loss: 415.9003715515137\n",
      "Epoch 44/1000, Loss: 415.40417098999023\n",
      "Epoch 45/1000, Loss: 417.56163024902344\n",
      "Epoch 46/1000, Loss: 413.84458923339844\n",
      "Epoch 47/1000, Loss: 416.833438873291\n",
      "Epoch 48/1000, Loss: 414.3152732849121\n",
      "Epoch 49/1000, Loss: 413.0310516357422\n",
      "Epoch 50/1000, Loss: 410.8034896850586\n",
      "Epoch 51/1000, Loss: 413.47104263305664\n",
      "Epoch 52/1000, Loss: 410.03155517578125\n",
      "Epoch 53/1000, Loss: 412.6568717956543\n",
      "Epoch 54/1000, Loss: 410.5953025817871\n",
      "Epoch 55/1000, Loss: 409.30724716186523\n",
      "Epoch 56/1000, Loss: 410.0155220031738\n",
      "Epoch 57/1000, Loss: 409.7576370239258\n",
      "Epoch 58/1000, Loss: 413.2566261291504\n",
      "Epoch 59/1000, Loss: 410.55003356933594\n",
      "Epoch 60/1000, Loss: 409.72920989990234\n",
      "Epoch 61/1000, Loss: 409.74532318115234\n",
      "Epoch 62/1000, Loss: 406.48414611816406\n",
      "Epoch 63/1000, Loss: 409.7415428161621\n",
      "Epoch 64/1000, Loss: 410.10345458984375\n",
      "Epoch 65/1000, Loss: 408.7925262451172\n",
      "Epoch 66/1000, Loss: 409.61595153808594\n",
      "Epoch 67/1000, Loss: 406.18368911743164\n",
      "Epoch 68/1000, Loss: 407.3267478942871\n",
      "Epoch 69/1000, Loss: 407.205078125\n",
      "Epoch 70/1000, Loss: 405.387638092041\n",
      "Epoch 71/1000, Loss: 406.8644714355469\n",
      "Epoch 72/1000, Loss: 407.0144920349121\n",
      "Epoch 73/1000, Loss: 404.9762420654297\n",
      "Epoch 74/1000, Loss: 401.3773956298828\n",
      "Epoch 75/1000, Loss: 404.898983001709\n",
      "Epoch 76/1000, Loss: 401.24402236938477\n",
      "Epoch 77/1000, Loss: 405.22064208984375\n",
      "Epoch 78/1000, Loss: 410.2742347717285\n",
      "Epoch 79/1000, Loss: 405.85336685180664\n",
      "Epoch 80/1000, Loss: 399.85407638549805\n",
      "Epoch 81/1000, Loss: 407.56157302856445\n",
      "Epoch 82/1000, Loss: 403.6509475708008\n",
      "Epoch 83/1000, Loss: 405.37378692626953\n",
      "Epoch 84/1000, Loss: 403.7850036621094\n",
      "Epoch 85/1000, Loss: 406.7175598144531\n",
      "Epoch 86/1000, Loss: 397.7491092681885\n",
      "Epoch 87/1000, Loss: 399.79199981689453\n",
      "Epoch 88/1000, Loss: 401.2602424621582\n",
      "Epoch 89/1000, Loss: 404.4051933288574\n",
      "Epoch 90/1000, Loss: 401.87229919433594\n",
      "Epoch 91/1000, Loss: 398.7820281982422\n",
      "Epoch 92/1000, Loss: 401.8166694641113\n",
      "Epoch 93/1000, Loss: 397.8260154724121\n",
      "Epoch 94/1000, Loss: 399.9006004333496\n",
      "Epoch 95/1000, Loss: 402.13669776916504\n",
      "Epoch 96/1000, Loss: 400.01848220825195\n",
      "Epoch 97/1000, Loss: 396.42352294921875\n",
      "Epoch 98/1000, Loss: 400.06533432006836\n",
      "Epoch 99/1000, Loss: 396.5650863647461\n",
      "Epoch 100/1000, Loss: 397.43873596191406\n",
      "Epoch 101/1000, Loss: 398.0062942504883\n",
      "Epoch 102/1000, Loss: 398.9896469116211\n",
      "Epoch 103/1000, Loss: 399.56360244750977\n",
      "Epoch 104/1000, Loss: 397.75371170043945\n",
      "Epoch 105/1000, Loss: 399.86805725097656\n",
      "Epoch 106/1000, Loss: 400.7965850830078\n",
      "Epoch 107/1000, Loss: 399.3352737426758\n",
      "Epoch 108/1000, Loss: 395.18088150024414\n",
      "Epoch 109/1000, Loss: 395.6127166748047\n",
      "Epoch 110/1000, Loss: 396.158145904541\n",
      "Epoch 111/1000, Loss: 396.52670669555664\n",
      "Epoch 112/1000, Loss: 399.2793731689453\n",
      "Epoch 113/1000, Loss: 397.49478912353516\n",
      "Epoch 114/1000, Loss: 396.02167892456055\n",
      "Epoch 115/1000, Loss: 396.5035095214844\n",
      "Epoch 116/1000, Loss: 394.85458755493164\n",
      "Epoch 117/1000, Loss: 394.2531547546387\n",
      "Epoch 118/1000, Loss: 396.5411186218262\n",
      "Epoch 119/1000, Loss: 392.4417037963867\n",
      "Epoch 120/1000, Loss: 395.475341796875\n",
      "Epoch 121/1000, Loss: 392.7977180480957\n",
      "Epoch 122/1000, Loss: 394.19744873046875\n",
      "Epoch 123/1000, Loss: 397.2945365905762\n",
      "Epoch 124/1000, Loss: 392.6248321533203\n",
      "Epoch 125/1000, Loss: 398.4742622375488\n",
      "Epoch 126/1000, Loss: 389.17512702941895\n",
      "Epoch 127/1000, Loss: 389.9802131652832\n",
      "Epoch 128/1000, Loss: 391.88827896118164\n",
      "Epoch 129/1000, Loss: 392.9771385192871\n",
      "Epoch 130/1000, Loss: 391.1512908935547\n",
      "Epoch 131/1000, Loss: 394.97435760498047\n",
      "Epoch 132/1000, Loss: 390.8669738769531\n",
      "Epoch 133/1000, Loss: 393.662296295166\n",
      "Epoch 134/1000, Loss: 393.21399688720703\n",
      "Epoch 135/1000, Loss: 393.6593017578125\n",
      "Epoch 136/1000, Loss: 391.9495506286621\n",
      "Epoch 137/1000, Loss: 391.7556838989258\n",
      "Epoch 138/1000, Loss: 391.11598205566406\n",
      "Epoch 139/1000, Loss: 394.5881233215332\n",
      "Epoch 140/1000, Loss: 391.96876525878906\n",
      "Epoch 141/1000, Loss: 390.4751091003418\n",
      "Epoch 142/1000, Loss: 390.37468338012695\n",
      "Epoch 143/1000, Loss: 394.8287467956543\n",
      "Epoch 144/1000, Loss: 392.4769058227539\n",
      "Epoch 145/1000, Loss: 395.51612091064453\n",
      "Epoch 146/1000, Loss: 387.8721466064453\n",
      "Epoch 147/1000, Loss: 389.9345474243164\n",
      "Epoch 148/1000, Loss: 389.7974052429199\n",
      "Epoch 149/1000, Loss: 390.3413887023926\n",
      "Epoch 150/1000, Loss: 391.7043571472168\n",
      "Epoch 151/1000, Loss: 392.39341735839844\n",
      "Epoch 152/1000, Loss: 389.0552291870117\n",
      "Epoch 153/1000, Loss: 388.20215606689453\n",
      "Epoch 154/1000, Loss: 391.04259490966797\n",
      "Epoch 155/1000, Loss: 391.5720520019531\n",
      "Epoch 156/1000, Loss: 385.82836151123047\n",
      "Epoch 157/1000, Loss: 388.4997253417969\n",
      "Epoch 158/1000, Loss: 391.3252067565918\n",
      "Epoch 159/1000, Loss: 386.07210540771484\n",
      "Epoch 160/1000, Loss: 387.36584854125977\n",
      "Epoch 161/1000, Loss: 389.8769950866699\n",
      "Epoch 162/1000, Loss: 388.61254501342773\n",
      "Epoch 163/1000, Loss: 382.2580089569092\n",
      "Epoch 164/1000, Loss: 392.11956787109375\n",
      "Epoch 165/1000, Loss: 387.52458572387695\n",
      "Epoch 166/1000, Loss: 388.19608879089355\n",
      "Epoch 167/1000, Loss: 386.47927474975586\n",
      "Epoch 168/1000, Loss: 386.8104553222656\n",
      "Epoch 169/1000, Loss: 383.66173934936523\n",
      "Epoch 170/1000, Loss: 390.02536392211914\n",
      "Epoch 171/1000, Loss: 387.3575019836426\n",
      "Epoch 172/1000, Loss: 387.92016220092773\n",
      "Epoch 173/1000, Loss: 387.654109954834\n",
      "Epoch 174/1000, Loss: 386.5454292297363\n",
      "Epoch 175/1000, Loss: 382.95567321777344\n",
      "Epoch 176/1000, Loss: 387.34514236450195\n",
      "Epoch 177/1000, Loss: 386.21935272216797\n",
      "Epoch 178/1000, Loss: 386.21896743774414\n",
      "Epoch 179/1000, Loss: 383.2699089050293\n",
      "Epoch 180/1000, Loss: 387.597843170166\n",
      "Epoch 181/1000, Loss: 384.0139694213867\n",
      "Epoch 182/1000, Loss: 383.14356231689453\n",
      "Epoch 183/1000, Loss: 383.7158889770508\n",
      "Epoch 184/1000, Loss: 385.96437072753906\n",
      "Epoch 185/1000, Loss: 385.711519241333\n",
      "Epoch 186/1000, Loss: 388.3432083129883\n",
      "Epoch 187/1000, Loss: 386.7889747619629\n",
      "Epoch 188/1000, Loss: 383.2328300476074\n",
      "Epoch 189/1000, Loss: 388.21259689331055\n",
      "Epoch 190/1000, Loss: 381.1951370239258\n",
      "Epoch 191/1000, Loss: 381.5591239929199\n",
      "Epoch 192/1000, Loss: 384.786922454834\n",
      "Epoch 193/1000, Loss: 380.4277763366699\n",
      "Epoch 194/1000, Loss: 378.76277923583984\n",
      "Epoch 195/1000, Loss: 385.3103332519531\n",
      "Epoch 196/1000, Loss: 382.2111701965332\n",
      "Epoch 197/1000, Loss: 381.3395767211914\n",
      "Epoch 198/1000, Loss: 386.0609664916992\n",
      "Epoch 199/1000, Loss: 383.1659622192383\n",
      "Epoch 200/1000, Loss: 383.60447692871094\n",
      "Epoch 201/1000, Loss: 380.0137481689453\n",
      "Epoch 202/1000, Loss: 382.89216232299805\n",
      "Epoch 203/1000, Loss: 380.52566146850586\n",
      "Epoch 204/1000, Loss: 381.0103015899658\n",
      "Epoch 205/1000, Loss: 382.4988136291504\n",
      "Epoch 206/1000, Loss: 381.12689208984375\n",
      "Epoch 207/1000, Loss: 377.6410083770752\n",
      "Epoch 208/1000, Loss: 381.9991645812988\n",
      "Epoch 209/1000, Loss: 380.064022064209\n",
      "Epoch 210/1000, Loss: 384.1465091705322\n",
      "Epoch 211/1000, Loss: 380.8372001647949\n",
      "Epoch 212/1000, Loss: 378.8025894165039\n",
      "Epoch 213/1000, Loss: 382.70098876953125\n",
      "Epoch 214/1000, Loss: 382.26781845092773\n",
      "Epoch 215/1000, Loss: 380.1858158111572\n",
      "Epoch 216/1000, Loss: 380.51166915893555\n",
      "Epoch 217/1000, Loss: 384.4330863952637\n",
      "Epoch 218/1000, Loss: 379.6762580871582\n",
      "Epoch 219/1000, Loss: 380.0510902404785\n",
      "Epoch 220/1000, Loss: 378.23447036743164\n",
      "Epoch 221/1000, Loss: 378.3534164428711\n",
      "Epoch 222/1000, Loss: 379.96138763427734\n",
      "Epoch 223/1000, Loss: 383.4035358428955\n",
      "Epoch 224/1000, Loss: 377.6331977844238\n",
      "Epoch 225/1000, Loss: 379.50867080688477\n",
      "Epoch 226/1000, Loss: 378.8814468383789\n",
      "Epoch 227/1000, Loss: 379.15086364746094\n",
      "Epoch 228/1000, Loss: 380.98277282714844\n",
      "Epoch 229/1000, Loss: 381.1946144104004\n",
      "Epoch 230/1000, Loss: 379.76988220214844\n",
      "Epoch 231/1000, Loss: 378.72607803344727\n",
      "Epoch 232/1000, Loss: 381.20105743408203\n",
      "Epoch 233/1000, Loss: 381.939998626709\n",
      "Epoch 234/1000, Loss: 377.3063774108887\n",
      "Epoch 235/1000, Loss: 377.3330421447754\n",
      "Epoch 236/1000, Loss: 381.18218994140625\n",
      "Epoch 237/1000, Loss: 376.99302673339844\n",
      "Epoch 238/1000, Loss: 377.5629081726074\n",
      "Epoch 239/1000, Loss: 375.63479232788086\n",
      "Epoch 240/1000, Loss: 374.6676902770996\n",
      "Epoch 241/1000, Loss: 378.6681480407715\n",
      "Epoch 242/1000, Loss: 375.5991516113281\n",
      "Epoch 243/1000, Loss: 377.56875228881836\n",
      "Epoch 244/1000, Loss: 379.1100082397461\n",
      "Epoch 245/1000, Loss: 378.227481842041\n",
      "Epoch 246/1000, Loss: 379.32901763916016\n",
      "Epoch 247/1000, Loss: 379.93670654296875\n",
      "Epoch 248/1000, Loss: 374.3158645629883\n",
      "Epoch 249/1000, Loss: 377.7359962463379\n",
      "Epoch 250/1000, Loss: 380.7830276489258\n",
      "Epoch 251/1000, Loss: 375.0166702270508\n",
      "Epoch 252/1000, Loss: 373.46990966796875\n",
      "Epoch 253/1000, Loss: 372.279109954834\n",
      "Epoch 254/1000, Loss: 372.36312103271484\n",
      "Epoch 255/1000, Loss: 379.1919174194336\n",
      "Epoch 256/1000, Loss: 372.97947883605957\n",
      "Epoch 257/1000, Loss: 377.6074409484863\n",
      "Epoch 258/1000, Loss: 374.5359687805176\n",
      "Epoch 259/1000, Loss: 378.0158042907715\n",
      "Epoch 260/1000, Loss: 373.7235336303711\n",
      "Epoch 261/1000, Loss: 374.42719650268555\n",
      "Epoch 262/1000, Loss: 378.6886329650879\n",
      "Epoch 263/1000, Loss: 372.2335548400879\n",
      "Epoch 264/1000, Loss: 374.0155372619629\n",
      "Epoch 265/1000, Loss: 374.35203552246094\n",
      "Epoch 266/1000, Loss: 367.77693939208984\n",
      "Epoch 267/1000, Loss: 371.84278869628906\n",
      "Epoch 268/1000, Loss: 372.48226165771484\n",
      "Epoch 269/1000, Loss: 374.46345138549805\n",
      "Epoch 270/1000, Loss: 371.56773376464844\n",
      "Epoch 271/1000, Loss: 372.88573455810547\n",
      "Epoch 272/1000, Loss: 373.806941986084\n",
      "Epoch 273/1000, Loss: 373.05339431762695\n",
      "Epoch 274/1000, Loss: 371.7859420776367\n",
      "Epoch 275/1000, Loss: 372.37504959106445\n",
      "Epoch 276/1000, Loss: 369.9523811340332\n",
      "Epoch 277/1000, Loss: 372.1499938964844\n",
      "Epoch 278/1000, Loss: 371.70051193237305\n",
      "Epoch 279/1000, Loss: 372.7742233276367\n",
      "Epoch 280/1000, Loss: 375.2871856689453\n",
      "Epoch 281/1000, Loss: 371.41230392456055\n",
      "Epoch 282/1000, Loss: 370.3506660461426\n",
      "Epoch 283/1000, Loss: 373.76538467407227\n",
      "Epoch 284/1000, Loss: 374.3420524597168\n",
      "Epoch 285/1000, Loss: 375.74549865722656\n",
      "Epoch 286/1000, Loss: 369.2694320678711\n",
      "Epoch 287/1000, Loss: 370.59628677368164\n",
      "Epoch 288/1000, Loss: 371.7004852294922\n",
      "Epoch 289/1000, Loss: 371.1729393005371\n",
      "Epoch 290/1000, Loss: 367.53196716308594\n",
      "Epoch 291/1000, Loss: 366.89757537841797\n",
      "Epoch 292/1000, Loss: 368.53752517700195\n",
      "Epoch 293/1000, Loss: 370.27928924560547\n",
      "Epoch 294/1000, Loss: 373.47381591796875\n",
      "Epoch 295/1000, Loss: 369.8169822692871\n",
      "Epoch 296/1000, Loss: 372.79076385498047\n",
      "Epoch 297/1000, Loss: 371.9043617248535\n",
      "Epoch 298/1000, Loss: 367.0338668823242\n",
      "Epoch 299/1000, Loss: 368.61316299438477\n",
      "Epoch 300/1000, Loss: 368.533935546875\n",
      "Epoch 301/1000, Loss: 367.8060417175293\n",
      "Epoch 302/1000, Loss: 368.7570915222168\n",
      "Epoch 303/1000, Loss: 364.69511795043945\n",
      "Epoch 304/1000, Loss: 371.4503364562988\n",
      "Epoch 305/1000, Loss: 371.6052055358887\n",
      "Epoch 306/1000, Loss: 368.2630271911621\n",
      "Epoch 307/1000, Loss: 371.80449867248535\n",
      "Epoch 308/1000, Loss: 370.9264678955078\n",
      "Epoch 309/1000, Loss: 368.1613121032715\n",
      "Epoch 310/1000, Loss: 369.77733993530273\n",
      "Epoch 311/1000, Loss: 365.4142303466797\n",
      "Epoch 312/1000, Loss: 367.57328605651855\n",
      "Epoch 313/1000, Loss: 369.5781669616699\n",
      "Epoch 314/1000, Loss: 368.5402297973633\n",
      "Epoch 315/1000, Loss: 365.51702880859375\n",
      "Epoch 316/1000, Loss: 366.86500358581543\n",
      "Epoch 317/1000, Loss: 367.22696685791016\n",
      "Epoch 318/1000, Loss: 367.8047065734863\n",
      "Epoch 319/1000, Loss: 367.46767807006836\n",
      "Epoch 320/1000, Loss: 366.3067207336426\n",
      "Epoch 321/1000, Loss: 371.57243728637695\n",
      "Epoch 322/1000, Loss: 369.87591552734375\n",
      "Epoch 323/1000, Loss: 367.40237045288086\n",
      "Epoch 324/1000, Loss: 364.34800720214844\n",
      "Epoch 325/1000, Loss: 362.18940925598145\n",
      "Epoch 326/1000, Loss: 365.29187774658203\n",
      "Epoch 327/1000, Loss: 368.1203308105469\n",
      "Epoch 328/1000, Loss: 366.1855354309082\n",
      "Epoch 329/1000, Loss: 367.4046001434326\n",
      "Epoch 330/1000, Loss: 364.79348373413086\n",
      "Epoch 331/1000, Loss: 363.5982131958008\n",
      "Epoch 332/1000, Loss: 369.7892150878906\n",
      "Epoch 333/1000, Loss: 362.9269599914551\n",
      "Epoch 334/1000, Loss: 366.4048900604248\n",
      "Epoch 335/1000, Loss: 361.19183921813965\n",
      "Epoch 336/1000, Loss: 366.98118591308594\n",
      "Epoch 337/1000, Loss: 364.6439208984375\n",
      "Epoch 338/1000, Loss: 368.9052543640137\n",
      "Epoch 339/1000, Loss: 361.0878028869629\n",
      "Epoch 340/1000, Loss: 361.874324798584\n",
      "Epoch 341/1000, Loss: 363.4100761413574\n",
      "Epoch 342/1000, Loss: 365.05830001831055\n",
      "Epoch 343/1000, Loss: 364.5522880554199\n",
      "Epoch 344/1000, Loss: 366.7754306793213\n",
      "Epoch 345/1000, Loss: 364.2755126953125\n",
      "Epoch 346/1000, Loss: 363.4889717102051\n",
      "Epoch 347/1000, Loss: 364.104434967041\n",
      "Epoch 348/1000, Loss: 365.711483001709\n",
      "Epoch 349/1000, Loss: 363.08027267456055\n",
      "Epoch 350/1000, Loss: 360.4281921386719\n",
      "Epoch 351/1000, Loss: 364.80952072143555\n",
      "Epoch 352/1000, Loss: 366.4644241333008\n",
      "Epoch 353/1000, Loss: 365.3282165527344\n",
      "Epoch 354/1000, Loss: 361.7703285217285\n",
      "Epoch 355/1000, Loss: 360.43859100341797\n",
      "Epoch 356/1000, Loss: 361.3330383300781\n",
      "Epoch 357/1000, Loss: 361.38522720336914\n",
      "Epoch 358/1000, Loss: 363.5704708099365\n",
      "Epoch 359/1000, Loss: 360.2200622558594\n",
      "Epoch 360/1000, Loss: 362.04347229003906\n",
      "Epoch 361/1000, Loss: 361.1522903442383\n",
      "Epoch 362/1000, Loss: 363.97086334228516\n",
      "Epoch 363/1000, Loss: 358.7742614746094\n",
      "Epoch 364/1000, Loss: 359.2447319030762\n",
      "Epoch 365/1000, Loss: 360.586950302124\n",
      "Epoch 366/1000, Loss: 360.54357147216797\n",
      "Epoch 367/1000, Loss: 353.42277431488037\n",
      "Epoch 368/1000, Loss: 360.0167045593262\n",
      "Epoch 369/1000, Loss: 363.4225730895996\n",
      "Epoch 370/1000, Loss: 360.5955352783203\n",
      "Epoch 371/1000, Loss: 361.61571502685547\n",
      "Epoch 372/1000, Loss: 362.37498664855957\n",
      "Epoch 373/1000, Loss: 358.37772369384766\n",
      "Epoch 374/1000, Loss: 363.93838119506836\n",
      "Epoch 375/1000, Loss: 362.0046195983887\n",
      "Epoch 376/1000, Loss: 360.92134857177734\n",
      "Epoch 377/1000, Loss: 359.77988052368164\n",
      "Epoch 378/1000, Loss: 356.30199241638184\n",
      "Epoch 379/1000, Loss: 362.66127586364746\n",
      "Epoch 380/1000, Loss: 363.84061431884766\n",
      "Epoch 381/1000, Loss: 362.0101509094238\n",
      "Epoch 382/1000, Loss: 360.3057861328125\n",
      "Epoch 383/1000, Loss: 360.39937591552734\n",
      "Epoch 384/1000, Loss: 356.2228469848633\n",
      "Epoch 385/1000, Loss: 360.4454917907715\n",
      "Epoch 386/1000, Loss: 362.2204399108887\n",
      "Epoch 387/1000, Loss: 357.08095359802246\n",
      "Epoch 388/1000, Loss: 361.6453857421875\n",
      "Epoch 389/1000, Loss: 360.08705139160156\n",
      "Epoch 390/1000, Loss: 363.0095500946045\n",
      "Epoch 391/1000, Loss: 361.03375244140625\n",
      "Epoch 392/1000, Loss: 352.8323612213135\n",
      "Epoch 393/1000, Loss: 357.1604881286621\n",
      "Epoch 394/1000, Loss: 361.153621673584\n",
      "Epoch 395/1000, Loss: 352.97657203674316\n",
      "Epoch 396/1000, Loss: 360.04128646850586\n",
      "Epoch 397/1000, Loss: 354.1930465698242\n",
      "Epoch 398/1000, Loss: 356.8266296386719\n",
      "Epoch 399/1000, Loss: 357.6019821166992\n",
      "Epoch 400/1000, Loss: 355.1836814880371\n",
      "Epoch 401/1000, Loss: 360.208553314209\n",
      "Epoch 402/1000, Loss: 357.0862274169922\n",
      "Epoch 403/1000, Loss: 362.6134281158447\n",
      "Epoch 404/1000, Loss: 356.34861755371094\n",
      "Epoch 405/1000, Loss: 352.1315002441406\n",
      "Epoch 406/1000, Loss: 356.02872467041016\n",
      "Epoch 407/1000, Loss: 359.89251708984375\n",
      "Epoch 408/1000, Loss: 354.82960510253906\n",
      "Epoch 409/1000, Loss: 358.5959072113037\n",
      "Epoch 410/1000, Loss: 356.13782501220703\n",
      "Epoch 411/1000, Loss: 356.2167549133301\n",
      "Epoch 412/1000, Loss: 355.558650970459\n",
      "Epoch 413/1000, Loss: 355.66843032836914\n",
      "Epoch 414/1000, Loss: 358.8243522644043\n",
      "Epoch 415/1000, Loss: 356.1624526977539\n",
      "Epoch 416/1000, Loss: 354.9318161010742\n",
      "Epoch 417/1000, Loss: 357.32607650756836\n",
      "Epoch 418/1000, Loss: 355.23527908325195\n",
      "Epoch 419/1000, Loss: 356.2802734375\n",
      "Epoch 420/1000, Loss: 354.13873291015625\n",
      "Epoch 421/1000, Loss: 350.9757652282715\n",
      "Epoch 422/1000, Loss: 355.04459381103516\n",
      "Epoch 423/1000, Loss: 357.64325523376465\n",
      "Epoch 424/1000, Loss: 356.33015060424805\n",
      "Epoch 425/1000, Loss: 358.1169128417969\n",
      "Epoch 426/1000, Loss: 352.63784408569336\n",
      "Epoch 427/1000, Loss: 356.58160400390625\n",
      "Epoch 428/1000, Loss: 356.40136528015137\n",
      "Epoch 429/1000, Loss: 354.36829566955566\n",
      "Epoch 430/1000, Loss: 356.06822204589844\n",
      "Epoch 431/1000, Loss: 353.9121894836426\n",
      "Epoch 432/1000, Loss: 357.0647201538086\n",
      "Epoch 433/1000, Loss: 348.7457733154297\n",
      "Epoch 434/1000, Loss: 351.1425437927246\n",
      "Epoch 435/1000, Loss: 354.71362686157227\n",
      "Epoch 436/1000, Loss: 350.7098922729492\n",
      "Epoch 437/1000, Loss: 357.10787200927734\n",
      "Epoch 438/1000, Loss: 353.6934623718262\n",
      "Epoch 439/1000, Loss: 353.03685760498047\n",
      "Epoch 440/1000, Loss: 348.2439136505127\n",
      "Epoch 441/1000, Loss: 352.2485885620117\n",
      "Epoch 442/1000, Loss: 352.6325225830078\n",
      "Epoch 443/1000, Loss: 352.5731658935547\n",
      "Epoch 444/1000, Loss: 350.86738204956055\n",
      "Epoch 445/1000, Loss: 353.17043685913086\n",
      "Epoch 446/1000, Loss: 353.5142822265625\n",
      "Epoch 447/1000, Loss: 350.1731986999512\n",
      "Epoch 448/1000, Loss: 354.2317810058594\n",
      "Epoch 449/1000, Loss: 354.3605270385742\n",
      "Epoch 450/1000, Loss: 354.58823013305664\n",
      "Epoch 451/1000, Loss: 353.47411346435547\n",
      "Epoch 452/1000, Loss: 353.1126956939697\n",
      "Epoch 453/1000, Loss: 355.3797950744629\n",
      "Epoch 454/1000, Loss: 353.0245704650879\n",
      "Epoch 455/1000, Loss: 349.42298126220703\n",
      "Epoch 456/1000, Loss: 350.72900390625\n",
      "Epoch 457/1000, Loss: 355.36777114868164\n",
      "Epoch 458/1000, Loss: 354.57941246032715\n",
      "Epoch 459/1000, Loss: 349.69176864624023\n",
      "Epoch 460/1000, Loss: 354.1763038635254\n",
      "Epoch 461/1000, Loss: 352.28525161743164\n",
      "Epoch 462/1000, Loss: 349.3202209472656\n",
      "Epoch 463/1000, Loss: 350.6793212890625\n",
      "Epoch 464/1000, Loss: 351.10679626464844\n",
      "Epoch 465/1000, Loss: 351.3899726867676\n",
      "Epoch 466/1000, Loss: 350.70544052124023\n",
      "Epoch 467/1000, Loss: 348.3699150085449\n",
      "Epoch 468/1000, Loss: 348.2838363647461\n",
      "Epoch 469/1000, Loss: 348.03103256225586\n",
      "Epoch 470/1000, Loss: 345.0885314941406\n",
      "Epoch 471/1000, Loss: 350.23208236694336\n",
      "Epoch 472/1000, Loss: 348.2865562438965\n",
      "Epoch 473/1000, Loss: 348.2839546203613\n",
      "Epoch 474/1000, Loss: 346.54431533813477\n",
      "Epoch 475/1000, Loss: 354.45988845825195\n",
      "Epoch 476/1000, Loss: 348.73030281066895\n",
      "Epoch 477/1000, Loss: 351.62447929382324\n",
      "Epoch 478/1000, Loss: 350.4700565338135\n",
      "Epoch 479/1000, Loss: 350.8735466003418\n",
      "Epoch 480/1000, Loss: 349.155948638916\n",
      "Epoch 481/1000, Loss: 346.1039810180664\n",
      "Epoch 482/1000, Loss: 345.88187980651855\n",
      "Epoch 483/1000, Loss: 350.03832817077637\n",
      "Epoch 484/1000, Loss: 346.2504425048828\n",
      "Epoch 485/1000, Loss: 349.4138298034668\n",
      "Epoch 486/1000, Loss: 347.0346794128418\n",
      "Epoch 487/1000, Loss: 350.15656661987305\n",
      "Epoch 488/1000, Loss: 350.0267524719238\n",
      "Epoch 489/1000, Loss: 346.1035461425781\n",
      "Epoch 490/1000, Loss: 346.0122241973877\n",
      "Epoch 491/1000, Loss: 348.95239639282227\n",
      "Epoch 492/1000, Loss: 348.1371192932129\n",
      "Epoch 493/1000, Loss: 347.05234146118164\n",
      "Epoch 494/1000, Loss: 346.1027183532715\n",
      "Epoch 495/1000, Loss: 348.3257026672363\n",
      "Epoch 496/1000, Loss: 346.9453315734863\n",
      "Epoch 497/1000, Loss: 347.4569320678711\n",
      "Epoch 498/1000, Loss: 346.09008979797363\n",
      "Epoch 499/1000, Loss: 345.9502944946289\n",
      "Epoch 500/1000, Loss: 345.04311752319336\n",
      "Epoch 501/1000, Loss: 344.3671340942383\n",
      "Epoch 502/1000, Loss: 348.54969024658203\n",
      "Epoch 503/1000, Loss: 346.76933670043945\n",
      "Epoch 504/1000, Loss: 349.6587142944336\n",
      "Epoch 505/1000, Loss: 348.2555274963379\n",
      "Epoch 506/1000, Loss: 344.0237579345703\n",
      "Epoch 507/1000, Loss: 348.84675216674805\n",
      "Epoch 508/1000, Loss: 345.54240226745605\n",
      "Epoch 509/1000, Loss: 346.61539459228516\n",
      "Epoch 510/1000, Loss: 346.8406276702881\n",
      "Epoch 511/1000, Loss: 341.89192962646484\n",
      "Epoch 512/1000, Loss: 350.28771209716797\n",
      "Epoch 513/1000, Loss: 349.06250762939453\n",
      "Epoch 514/1000, Loss: 344.47021865844727\n",
      "Epoch 515/1000, Loss: 346.9411449432373\n",
      "Epoch 516/1000, Loss: 346.959415435791\n",
      "Epoch 517/1000, Loss: 347.0821304321289\n",
      "Epoch 518/1000, Loss: 345.8861961364746\n",
      "Epoch 519/1000, Loss: 347.2252540588379\n",
      "Epoch 520/1000, Loss: 344.2317352294922\n",
      "Epoch 521/1000, Loss: 345.47998046875\n",
      "Epoch 522/1000, Loss: 346.66299629211426\n",
      "Epoch 523/1000, Loss: 339.34242820739746\n",
      "Epoch 524/1000, Loss: 344.8151264190674\n",
      "Epoch 525/1000, Loss: 343.92266845703125\n",
      "Epoch 526/1000, Loss: 343.80126953125\n",
      "Epoch 527/1000, Loss: 342.3605041503906\n",
      "Epoch 528/1000, Loss: 346.56295585632324\n",
      "Epoch 529/1000, Loss: 342.9438705444336\n",
      "Epoch 530/1000, Loss: 339.9258441925049\n",
      "Epoch 531/1000, Loss: 342.0376892089844\n",
      "Epoch 532/1000, Loss: 338.5586986541748\n",
      "Epoch 533/1000, Loss: 343.41455078125\n",
      "Epoch 534/1000, Loss: 341.43915939331055\n",
      "Epoch 535/1000, Loss: 344.20973205566406\n",
      "Epoch 536/1000, Loss: 342.5143356323242\n",
      "Epoch 537/1000, Loss: 345.88045501708984\n",
      "Epoch 538/1000, Loss: 341.16352462768555\n",
      "Epoch 539/1000, Loss: 341.1333885192871\n",
      "Epoch 540/1000, Loss: 342.8976173400879\n",
      "Epoch 541/1000, Loss: 342.3802947998047\n",
      "Epoch 542/1000, Loss: 343.63500022888184\n",
      "Epoch 543/1000, Loss: 335.59000396728516\n",
      "Epoch 544/1000, Loss: 343.847843170166\n",
      "Epoch 545/1000, Loss: 340.8604907989502\n",
      "Epoch 546/1000, Loss: 339.8466548919678\n",
      "Epoch 547/1000, Loss: 344.01355743408203\n",
      "Epoch 548/1000, Loss: 343.3043518066406\n",
      "Epoch 549/1000, Loss: 336.9919033050537\n",
      "Epoch 550/1000, Loss: 341.01030349731445\n",
      "Epoch 551/1000, Loss: 342.66445541381836\n",
      "Epoch 552/1000, Loss: 343.98493003845215\n",
      "Epoch 553/1000, Loss: 341.0376625061035\n",
      "Epoch 554/1000, Loss: 341.8267364501953\n",
      "Epoch 555/1000, Loss: 343.3024654388428\n",
      "Epoch 556/1000, Loss: 342.0543098449707\n",
      "Epoch 557/1000, Loss: 340.8045539855957\n",
      "Epoch 558/1000, Loss: 342.39818954467773\n",
      "Epoch 559/1000, Loss: 340.28520011901855\n",
      "Epoch 560/1000, Loss: 341.66929626464844\n",
      "Epoch 561/1000, Loss: 343.7344627380371\n",
      "Epoch 562/1000, Loss: 341.0664825439453\n",
      "Epoch 563/1000, Loss: 334.90858268737793\n",
      "Epoch 564/1000, Loss: 336.59334564208984\n",
      "Epoch 565/1000, Loss: 340.9003562927246\n",
      "Epoch 566/1000, Loss: 338.65636444091797\n",
      "Epoch 567/1000, Loss: 338.9757995605469\n",
      "Epoch 568/1000, Loss: 340.25872230529785\n",
      "Epoch 569/1000, Loss: 341.21947479248047\n",
      "Epoch 570/1000, Loss: 336.9343376159668\n",
      "Epoch 571/1000, Loss: 339.7482681274414\n",
      "Epoch 572/1000, Loss: 338.32702255249023\n",
      "Epoch 573/1000, Loss: 337.98453521728516\n",
      "Epoch 574/1000, Loss: 339.38500213623047\n",
      "Epoch 575/1000, Loss: 336.6170997619629\n",
      "Epoch 576/1000, Loss: 338.2379341125488\n",
      "Epoch 577/1000, Loss: 341.29602813720703\n",
      "Epoch 578/1000, Loss: 337.07868576049805\n",
      "Epoch 579/1000, Loss: 336.39766693115234\n",
      "Epoch 580/1000, Loss: 337.0516548156738\n",
      "Epoch 581/1000, Loss: 341.79830741882324\n",
      "Epoch 582/1000, Loss: 339.65724182128906\n",
      "Epoch 583/1000, Loss: 339.61921310424805\n",
      "Epoch 584/1000, Loss: 334.817813873291\n",
      "Epoch 585/1000, Loss: 334.4939785003662\n",
      "Epoch 586/1000, Loss: 335.56593322753906\n",
      "Epoch 587/1000, Loss: 335.8675079345703\n",
      "Epoch 588/1000, Loss: 332.99401092529297\n",
      "Epoch 589/1000, Loss: 338.4109344482422\n",
      "Epoch 590/1000, Loss: 338.1836967468262\n",
      "Epoch 591/1000, Loss: 336.63531494140625\n",
      "Epoch 592/1000, Loss: 336.8248481750488\n",
      "Epoch 593/1000, Loss: 333.69239807128906\n",
      "Epoch 594/1000, Loss: 334.11666679382324\n",
      "Epoch 595/1000, Loss: 336.3025016784668\n",
      "Epoch 596/1000, Loss: 336.7672748565674\n",
      "Epoch 597/1000, Loss: 335.5632019042969\n",
      "Epoch 598/1000, Loss: 337.3919982910156\n",
      "Epoch 599/1000, Loss: 337.87145042419434\n",
      "Epoch 600/1000, Loss: 336.97632598876953\n",
      "Epoch 601/1000, Loss: 335.53150177001953\n",
      "Epoch 602/1000, Loss: 339.00677490234375\n",
      "Epoch 603/1000, Loss: 337.93373107910156\n",
      "Epoch 604/1000, Loss: 335.52894592285156\n",
      "Epoch 605/1000, Loss: 335.1620063781738\n",
      "Epoch 606/1000, Loss: 333.46598052978516\n",
      "Epoch 607/1000, Loss: 335.664363861084\n",
      "Epoch 608/1000, Loss: 336.6238422393799\n",
      "Epoch 609/1000, Loss: 336.2089080810547\n",
      "Epoch 610/1000, Loss: 336.52711486816406\n",
      "Epoch 611/1000, Loss: 335.9655342102051\n",
      "Epoch 612/1000, Loss: 332.09154891967773\n",
      "Epoch 613/1000, Loss: 333.03067779541016\n",
      "Epoch 614/1000, Loss: 334.6721134185791\n",
      "Epoch 615/1000, Loss: 336.68934059143066\n",
      "Epoch 616/1000, Loss: 331.6862506866455\n",
      "Epoch 617/1000, Loss: 332.995849609375\n",
      "Epoch 618/1000, Loss: 334.6084518432617\n",
      "Epoch 619/1000, Loss: 331.23270988464355\n",
      "Epoch 620/1000, Loss: 334.02803802490234\n",
      "Epoch 621/1000, Loss: 330.18711853027344\n",
      "Epoch 622/1000, Loss: 336.5422058105469\n",
      "Epoch 623/1000, Loss: 333.605073928833\n",
      "Epoch 624/1000, Loss: 335.1911220550537\n",
      "Epoch 625/1000, Loss: 333.9778480529785\n",
      "Epoch 626/1000, Loss: 330.74504470825195\n",
      "Epoch 627/1000, Loss: 335.78357696533203\n",
      "Epoch 628/1000, Loss: 329.67263412475586\n",
      "Epoch 629/1000, Loss: 335.0116901397705\n",
      "Epoch 630/1000, Loss: 330.9979553222656\n",
      "Epoch 631/1000, Loss: 328.71271324157715\n",
      "Epoch 632/1000, Loss: 335.39403533935547\n",
      "Epoch 633/1000, Loss: 336.34572982788086\n",
      "Epoch 634/1000, Loss: 330.5804252624512\n",
      "Epoch 635/1000, Loss: 332.14076232910156\n",
      "Epoch 636/1000, Loss: 328.82699966430664\n",
      "Epoch 637/1000, Loss: 331.60985946655273\n",
      "Epoch 638/1000, Loss: 331.17197036743164\n",
      "Epoch 639/1000, Loss: 333.4811248779297\n",
      "Epoch 640/1000, Loss: 333.4012031555176\n",
      "Epoch 641/1000, Loss: 330.71197509765625\n",
      "Epoch 642/1000, Loss: 329.7750244140625\n",
      "Epoch 643/1000, Loss: 325.9794178009033\n",
      "Epoch 644/1000, Loss: 326.3756446838379\n",
      "Epoch 645/1000, Loss: 327.71295166015625\n",
      "Epoch 646/1000, Loss: 330.00830078125\n",
      "Epoch 647/1000, Loss: 335.1804542541504\n",
      "Epoch 648/1000, Loss: 332.5250549316406\n",
      "Epoch 649/1000, Loss: 334.27723121643066\n",
      "Epoch 650/1000, Loss: 330.6930694580078\n",
      "Epoch 651/1000, Loss: 330.3269233703613\n",
      "Epoch 652/1000, Loss: 330.6365718841553\n",
      "Epoch 653/1000, Loss: 329.57982635498047\n",
      "Epoch 654/1000, Loss: 327.6870594024658\n",
      "Epoch 655/1000, Loss: 328.50920486450195\n",
      "Epoch 656/1000, Loss: 333.3642692565918\n",
      "Epoch 657/1000, Loss: 331.03462409973145\n",
      "Epoch 658/1000, Loss: 330.79762268066406\n",
      "Epoch 659/1000, Loss: 329.4034767150879\n",
      "Epoch 660/1000, Loss: 333.3335361480713\n",
      "Epoch 661/1000, Loss: 326.5250988006592\n",
      "Epoch 662/1000, Loss: 329.6842803955078\n",
      "Epoch 663/1000, Loss: 326.7014961242676\n",
      "Epoch 664/1000, Loss: 328.40942764282227\n",
      "Epoch 665/1000, Loss: 332.26428413391113\n",
      "Epoch 666/1000, Loss: 328.581600189209\n",
      "Epoch 667/1000, Loss: 328.99875259399414\n",
      "Epoch 668/1000, Loss: 326.94462966918945\n",
      "Epoch 669/1000, Loss: 326.3891944885254\n",
      "Epoch 670/1000, Loss: 329.25634956359863\n",
      "Epoch 671/1000, Loss: 328.05006408691406\n",
      "Epoch 672/1000, Loss: 329.2473335266113\n",
      "Epoch 673/1000, Loss: 326.953161239624\n",
      "Epoch 674/1000, Loss: 327.08576583862305\n",
      "Epoch 675/1000, Loss: 324.5083713531494\n",
      "Epoch 676/1000, Loss: 324.49199295043945\n",
      "Epoch 677/1000, Loss: 327.72149658203125\n",
      "Epoch 678/1000, Loss: 327.66212463378906\n",
      "Epoch 679/1000, Loss: 325.4296646118164\n",
      "Epoch 680/1000, Loss: 329.7593078613281\n",
      "Epoch 681/1000, Loss: 324.8187427520752\n",
      "Epoch 682/1000, Loss: 324.65301513671875\n",
      "Epoch 683/1000, Loss: 324.8751335144043\n",
      "Epoch 684/1000, Loss: 327.6912307739258\n",
      "Epoch 685/1000, Loss: 324.5878505706787\n",
      "Epoch 686/1000, Loss: 327.7945919036865\n",
      "Epoch 687/1000, Loss: 324.7418689727783\n",
      "Epoch 688/1000, Loss: 327.229455947876\n",
      "Epoch 689/1000, Loss: 320.91689109802246\n",
      "Epoch 690/1000, Loss: 330.23168563842773\n",
      "Epoch 691/1000, Loss: 325.52474212646484\n",
      "Epoch 692/1000, Loss: 326.4570198059082\n",
      "Epoch 693/1000, Loss: 327.0772075653076\n",
      "Epoch 694/1000, Loss: 324.2014808654785\n",
      "Epoch 695/1000, Loss: 324.5217933654785\n",
      "Epoch 696/1000, Loss: 324.0776767730713\n",
      "Epoch 697/1000, Loss: 323.472562789917\n",
      "Epoch 698/1000, Loss: 325.92222023010254\n",
      "Epoch 699/1000, Loss: 324.72998809814453\n",
      "Epoch 700/1000, Loss: 323.0118942260742\n",
      "Epoch 701/1000, Loss: 324.19103050231934\n",
      "Epoch 702/1000, Loss: 325.3541202545166\n",
      "Epoch 703/1000, Loss: 322.3757019042969\n",
      "Epoch 704/1000, Loss: 324.75881004333496\n",
      "Epoch 705/1000, Loss: 322.13965225219727\n",
      "Epoch 706/1000, Loss: 320.13752937316895\n",
      "Epoch 707/1000, Loss: 324.1927089691162\n",
      "Epoch 708/1000, Loss: 320.2420482635498\n",
      "Epoch 709/1000, Loss: 328.69803047180176\n",
      "Epoch 710/1000, Loss: 322.34194564819336\n",
      "Epoch 711/1000, Loss: 324.3058490753174\n",
      "Epoch 712/1000, Loss: 323.53160095214844\n",
      "Epoch 713/1000, Loss: 322.1209182739258\n",
      "Epoch 714/1000, Loss: 324.49719619750977\n",
      "Epoch 715/1000, Loss: 327.02577209472656\n",
      "Epoch 716/1000, Loss: 323.9608325958252\n",
      "Epoch 717/1000, Loss: 323.2944583892822\n",
      "Epoch 718/1000, Loss: 323.0574493408203\n",
      "Epoch 719/1000, Loss: 326.12736320495605\n",
      "Epoch 720/1000, Loss: 323.3022880554199\n",
      "Epoch 721/1000, Loss: 319.6631374359131\n",
      "Epoch 722/1000, Loss: 324.5652618408203\n",
      "Epoch 723/1000, Loss: 322.394811630249\n",
      "Epoch 724/1000, Loss: 319.8894786834717\n",
      "Epoch 725/1000, Loss: 322.65051078796387\n",
      "Epoch 726/1000, Loss: 322.52439308166504\n",
      "Epoch 727/1000, Loss: 323.2884998321533\n",
      "Epoch 728/1000, Loss: 320.70361328125\n",
      "Epoch 729/1000, Loss: 322.27409172058105\n",
      "Epoch 730/1000, Loss: 318.5622310638428\n",
      "Epoch 731/1000, Loss: 321.42119216918945\n",
      "Epoch 732/1000, Loss: 321.0247440338135\n",
      "Epoch 733/1000, Loss: 320.30034255981445\n",
      "Epoch 734/1000, Loss: 319.0913200378418\n",
      "Epoch 735/1000, Loss: 316.37140464782715\n",
      "Epoch 736/1000, Loss: 321.59424209594727\n",
      "Epoch 737/1000, Loss: 320.74199867248535\n",
      "Epoch 738/1000, Loss: 320.45502853393555\n",
      "Epoch 739/1000, Loss: 319.374324798584\n",
      "Epoch 740/1000, Loss: 320.31277084350586\n",
      "Epoch 741/1000, Loss: 322.65829277038574\n",
      "Epoch 742/1000, Loss: 324.1062469482422\n",
      "Epoch 743/1000, Loss: 318.96046257019043\n",
      "Epoch 744/1000, Loss: 318.9006156921387\n",
      "Epoch 745/1000, Loss: 322.14971923828125\n",
      "Epoch 746/1000, Loss: 317.4360866546631\n",
      "Epoch 747/1000, Loss: 320.7456645965576\n",
      "Epoch 748/1000, Loss: 317.4854545593262\n",
      "Epoch 749/1000, Loss: 319.1205406188965\n",
      "Epoch 750/1000, Loss: 319.7567844390869\n",
      "Epoch 751/1000, Loss: 320.44564056396484\n",
      "Epoch 752/1000, Loss: 320.93152618408203\n",
      "Epoch 753/1000, Loss: 322.2208003997803\n",
      "Epoch 754/1000, Loss: 320.1937065124512\n",
      "Epoch 755/1000, Loss: 319.10245513916016\n",
      "Epoch 756/1000, Loss: 317.3834648132324\n",
      "Epoch 757/1000, Loss: 317.7293815612793\n",
      "Epoch 758/1000, Loss: 316.0342845916748\n",
      "Epoch 759/1000, Loss: 320.42313385009766\n",
      "Epoch 760/1000, Loss: 316.2409553527832\n",
      "Epoch 761/1000, Loss: 317.699857711792\n",
      "Epoch 762/1000, Loss: 316.929235458374\n",
      "Epoch 763/1000, Loss: 317.9851722717285\n",
      "Epoch 764/1000, Loss: 318.56007385253906\n",
      "Epoch 765/1000, Loss: 320.2585754394531\n",
      "Epoch 766/1000, Loss: 318.2457695007324\n",
      "Epoch 767/1000, Loss: 320.05853271484375\n",
      "Epoch 768/1000, Loss: 318.3147277832031\n",
      "Epoch 769/1000, Loss: 316.6445541381836\n",
      "Epoch 770/1000, Loss: 318.0210380554199\n",
      "Epoch 771/1000, Loss: 315.30132484436035\n",
      "Epoch 772/1000, Loss: 315.4643745422363\n",
      "Epoch 773/1000, Loss: 315.9879264831543\n",
      "Epoch 774/1000, Loss: 315.4019889831543\n",
      "Epoch 775/1000, Loss: 320.3880081176758\n",
      "Epoch 776/1000, Loss: 318.3832530975342\n",
      "Epoch 777/1000, Loss: 314.4871234893799\n",
      "Epoch 778/1000, Loss: 314.21027183532715\n",
      "Epoch 779/1000, Loss: 314.15155029296875\n",
      "Epoch 780/1000, Loss: 315.2073440551758\n",
      "Epoch 781/1000, Loss: 315.18656158447266\n",
      "Epoch 782/1000, Loss: 313.64753913879395\n",
      "Epoch 783/1000, Loss: 318.19755363464355\n",
      "Epoch 784/1000, Loss: 316.4375743865967\n",
      "Epoch 785/1000, Loss: 315.06485748291016\n",
      "Epoch 786/1000, Loss: 317.4765815734863\n",
      "Epoch 787/1000, Loss: 313.33391761779785\n",
      "Epoch 788/1000, Loss: 313.75658226013184\n",
      "Epoch 789/1000, Loss: 317.4359664916992\n",
      "Epoch 790/1000, Loss: 317.86308097839355\n",
      "Epoch 791/1000, Loss: 312.8133659362793\n",
      "Epoch 792/1000, Loss: 316.6069984436035\n",
      "Epoch 793/1000, Loss: 313.2733573913574\n",
      "Epoch 794/1000, Loss: 316.97744369506836\n",
      "Epoch 795/1000, Loss: 316.5777702331543\n",
      "Epoch 796/1000, Loss: 316.1817970275879\n",
      "Epoch 797/1000, Loss: 314.52895736694336\n",
      "Epoch 798/1000, Loss: 311.1428813934326\n",
      "Epoch 799/1000, Loss: 314.15172958374023\n",
      "Epoch 800/1000, Loss: 314.677827835083\n",
      "Epoch 801/1000, Loss: 313.83068466186523\n",
      "Epoch 802/1000, Loss: 312.59585189819336\n",
      "Epoch 803/1000, Loss: 313.1928291320801\n",
      "Epoch 804/1000, Loss: 311.7478733062744\n",
      "Epoch 805/1000, Loss: 315.68339920043945\n",
      "Epoch 806/1000, Loss: 316.3938331604004\n",
      "Epoch 807/1000, Loss: 313.0305290222168\n",
      "Epoch 808/1000, Loss: 312.5642623901367\n",
      "Epoch 809/1000, Loss: 309.82616233825684\n",
      "Epoch 810/1000, Loss: 312.5691165924072\n",
      "Epoch 811/1000, Loss: 317.58535385131836\n",
      "Epoch 812/1000, Loss: 310.67828369140625\n",
      "Epoch 813/1000, Loss: 315.9964656829834\n",
      "Epoch 814/1000, Loss: 311.2897834777832\n",
      "Epoch 815/1000, Loss: 314.0051918029785\n",
      "Epoch 816/1000, Loss: 312.7931709289551\n",
      "Epoch 817/1000, Loss: 311.6413974761963\n",
      "Epoch 818/1000, Loss: 313.7374382019043\n",
      "Epoch 819/1000, Loss: 315.08932876586914\n",
      "Epoch 820/1000, Loss: 307.7783794403076\n",
      "Epoch 821/1000, Loss: 313.50235748291016\n",
      "Epoch 822/1000, Loss: 314.49646759033203\n",
      "Epoch 823/1000, Loss: 312.07422065734863\n",
      "Epoch 824/1000, Loss: 310.50293350219727\n",
      "Epoch 825/1000, Loss: 312.12213134765625\n",
      "Epoch 826/1000, Loss: 313.3906307220459\n",
      "Epoch 827/1000, Loss: 310.43208503723145\n",
      "Epoch 828/1000, Loss: 313.31465911865234\n",
      "Epoch 829/1000, Loss: 313.98641777038574\n",
      "Epoch 830/1000, Loss: 312.0755252838135\n",
      "Epoch 831/1000, Loss: 311.54461097717285\n",
      "Epoch 832/1000, Loss: 309.73452377319336\n",
      "Epoch 833/1000, Loss: 310.78335189819336\n",
      "Epoch 834/1000, Loss: 310.01599502563477\n",
      "Epoch 835/1000, Loss: 311.1115493774414\n",
      "Epoch 836/1000, Loss: 310.5931816101074\n",
      "Epoch 837/1000, Loss: 311.3931350708008\n",
      "Epoch 838/1000, Loss: 313.02403831481934\n",
      "Epoch 839/1000, Loss: 308.92981910705566\n",
      "Epoch 840/1000, Loss: 311.77845001220703\n",
      "Epoch 841/1000, Loss: 309.80249977111816\n",
      "Epoch 842/1000, Loss: 313.1682376861572\n",
      "Epoch 843/1000, Loss: 305.6905155181885\n",
      "Epoch 844/1000, Loss: 307.55989265441895\n",
      "Epoch 845/1000, Loss: 306.41394233703613\n",
      "Epoch 846/1000, Loss: 305.92580223083496\n",
      "Epoch 847/1000, Loss: 310.44121170043945\n",
      "Epoch 848/1000, Loss: 312.21603775024414\n",
      "Epoch 849/1000, Loss: 311.3456382751465\n",
      "Epoch 850/1000, Loss: 309.4622573852539\n",
      "Epoch 851/1000, Loss: 308.1802291870117\n",
      "Epoch 852/1000, Loss: 308.8208999633789\n",
      "Epoch 853/1000, Loss: 313.4149646759033\n",
      "Epoch 854/1000, Loss: 306.41661262512207\n",
      "Epoch 855/1000, Loss: 307.463680267334\n",
      "Epoch 856/1000, Loss: 304.88841819763184\n",
      "Epoch 857/1000, Loss: 308.22835540771484\n",
      "Epoch 858/1000, Loss: 310.25662422180176\n",
      "Epoch 859/1000, Loss: 308.9717540740967\n",
      "Epoch 860/1000, Loss: 309.7899227142334\n",
      "Epoch 861/1000, Loss: 305.5703754425049\n",
      "Epoch 862/1000, Loss: 306.6720886230469\n",
      "Epoch 863/1000, Loss: 309.13478088378906\n",
      "Epoch 864/1000, Loss: 306.1597385406494\n",
      "Epoch 865/1000, Loss: 305.2983150482178\n",
      "Epoch 866/1000, Loss: 308.2769298553467\n",
      "Epoch 867/1000, Loss: 306.0380115509033\n",
      "Epoch 868/1000, Loss: 307.63906478881836\n",
      "Epoch 869/1000, Loss: 304.55032539367676\n",
      "Epoch 870/1000, Loss: 308.42039489746094\n",
      "Epoch 871/1000, Loss: 306.1403102874756\n",
      "Epoch 872/1000, Loss: 306.7825450897217\n",
      "Epoch 873/1000, Loss: 308.7187900543213\n",
      "Epoch 874/1000, Loss: 306.91958808898926\n",
      "Epoch 875/1000, Loss: 304.4473533630371\n",
      "Epoch 876/1000, Loss: 306.64581298828125\n",
      "Epoch 877/1000, Loss: 306.07925605773926\n",
      "Epoch 878/1000, Loss: 307.3307228088379\n",
      "Epoch 879/1000, Loss: 307.44078254699707\n",
      "Epoch 880/1000, Loss: 307.4386215209961\n",
      "Epoch 881/1000, Loss: 303.4242820739746\n",
      "Epoch 882/1000, Loss: 306.10697746276855\n",
      "Epoch 883/1000, Loss: 307.30060386657715\n",
      "Epoch 884/1000, Loss: 309.3560028076172\n",
      "Epoch 885/1000, Loss: 305.96555519104004\n",
      "Epoch 886/1000, Loss: 305.6998538970947\n",
      "Epoch 887/1000, Loss: 306.4416084289551\n",
      "Epoch 888/1000, Loss: 305.0403594970703\n",
      "Epoch 889/1000, Loss: 308.0881290435791\n",
      "Epoch 890/1000, Loss: 305.84204483032227\n",
      "Epoch 891/1000, Loss: 304.8356113433838\n",
      "Epoch 892/1000, Loss: 304.0686626434326\n",
      "Epoch 893/1000, Loss: 305.416690826416\n",
      "Epoch 894/1000, Loss: 302.11234855651855\n",
      "Epoch 895/1000, Loss: 302.8358840942383\n",
      "Epoch 896/1000, Loss: 301.7680492401123\n",
      "Epoch 897/1000, Loss: 302.014310836792\n",
      "Epoch 898/1000, Loss: 304.4535884857178\n",
      "Epoch 899/1000, Loss: 306.0654296875\n",
      "Epoch 900/1000, Loss: 302.9945487976074\n",
      "Epoch 901/1000, Loss: 303.6548442840576\n",
      "Epoch 902/1000, Loss: 307.7315330505371\n",
      "Epoch 903/1000, Loss: 305.5548038482666\n",
      "Epoch 904/1000, Loss: 300.930570602417\n",
      "Epoch 905/1000, Loss: 304.3930950164795\n",
      "Epoch 906/1000, Loss: 303.0339584350586\n",
      "Epoch 907/1000, Loss: 302.90048599243164\n",
      "Epoch 908/1000, Loss: 304.5259666442871\n",
      "Epoch 909/1000, Loss: 302.6262454986572\n",
      "Epoch 910/1000, Loss: 304.482421875\n",
      "Epoch 911/1000, Loss: 303.8779830932617\n",
      "Epoch 912/1000, Loss: 305.1958770751953\n",
      "Epoch 913/1000, Loss: 304.28100204467773\n",
      "Epoch 914/1000, Loss: 301.20919609069824\n",
      "Epoch 915/1000, Loss: 302.303373336792\n",
      "Epoch 916/1000, Loss: 301.13233757019043\n",
      "Epoch 917/1000, Loss: 303.3991870880127\n",
      "Epoch 918/1000, Loss: 303.2225093841553\n",
      "Epoch 919/1000, Loss: 299.8812732696533\n",
      "Epoch 920/1000, Loss: 301.71596908569336\n",
      "Epoch 921/1000, Loss: 302.1498050689697\n",
      "Epoch 922/1000, Loss: 297.85757637023926\n",
      "Epoch 923/1000, Loss: 304.9510250091553\n",
      "Epoch 924/1000, Loss: 300.3847961425781\n",
      "Epoch 925/1000, Loss: 298.1411781311035\n",
      "Epoch 926/1000, Loss: 302.9570827484131\n",
      "Epoch 927/1000, Loss: 302.4998779296875\n",
      "Epoch 928/1000, Loss: 302.6790599822998\n",
      "Epoch 929/1000, Loss: 299.110408782959\n",
      "Epoch 930/1000, Loss: 300.80865478515625\n",
      "Epoch 931/1000, Loss: 300.4788131713867\n",
      "Epoch 932/1000, Loss: 301.89512634277344\n",
      "Epoch 933/1000, Loss: 304.2860622406006\n",
      "Epoch 934/1000, Loss: 303.6426010131836\n",
      "Epoch 935/1000, Loss: 299.6536750793457\n",
      "Epoch 936/1000, Loss: 302.434757232666\n",
      "Epoch 937/1000, Loss: 297.66473960876465\n",
      "Epoch 938/1000, Loss: 303.59569931030273\n",
      "Epoch 939/1000, Loss: 299.07383728027344\n",
      "Epoch 940/1000, Loss: 300.5880889892578\n",
      "Epoch 941/1000, Loss: 303.13425636291504\n",
      "Epoch 942/1000, Loss: 299.50555419921875\n",
      "Epoch 943/1000, Loss: 300.7966480255127\n",
      "Epoch 944/1000, Loss: 303.1837272644043\n",
      "Epoch 945/1000, Loss: 300.2030429840088\n",
      "Epoch 946/1000, Loss: 296.3518314361572\n",
      "Epoch 947/1000, Loss: 301.72303199768066\n",
      "Epoch 948/1000, Loss: 297.4223232269287\n",
      "Epoch 949/1000, Loss: 299.57725524902344\n",
      "Epoch 950/1000, Loss: 297.1556797027588\n",
      "Epoch 951/1000, Loss: 300.84363555908203\n",
      "Epoch 952/1000, Loss: 299.07364654541016\n",
      "Epoch 953/1000, Loss: 297.32118797302246\n",
      "Epoch 954/1000, Loss: 295.6086311340332\n",
      "Epoch 955/1000, Loss: 299.29425621032715\n",
      "Epoch 956/1000, Loss: 300.4960460662842\n",
      "Epoch 957/1000, Loss: 299.99937438964844\n",
      "Epoch 958/1000, Loss: 297.48657989501953\n",
      "Epoch 959/1000, Loss: 299.2457904815674\n",
      "Epoch 960/1000, Loss: 298.1804027557373\n",
      "Epoch 961/1000, Loss: 298.66592025756836\n",
      "Epoch 962/1000, Loss: 295.81602478027344\n",
      "Epoch 963/1000, Loss: 296.6863136291504\n",
      "Epoch 964/1000, Loss: 299.01654624938965\n",
      "Epoch 965/1000, Loss: 297.127742767334\n",
      "Epoch 966/1000, Loss: 299.21538734436035\n",
      "Epoch 967/1000, Loss: 292.26999855041504\n",
      "Epoch 968/1000, Loss: 295.06054878234863\n",
      "Epoch 969/1000, Loss: 295.5883045196533\n",
      "Epoch 970/1000, Loss: 298.38390922546387\n",
      "Epoch 971/1000, Loss: 295.42932319641113\n",
      "Epoch 972/1000, Loss: 296.9031734466553\n",
      "Epoch 973/1000, Loss: 294.36918449401855\n",
      "Epoch 974/1000, Loss: 300.7835216522217\n",
      "Epoch 975/1000, Loss: 298.0931701660156\n",
      "Epoch 976/1000, Loss: 300.536828994751\n",
      "Epoch 977/1000, Loss: 293.46545791625977\n",
      "Epoch 978/1000, Loss: 298.1395893096924\n",
      "Epoch 979/1000, Loss: 294.6993923187256\n",
      "Epoch 980/1000, Loss: 295.29540061950684\n",
      "Epoch 981/1000, Loss: 290.49065017700195\n",
      "Epoch 982/1000, Loss: 293.1264114379883\n",
      "Epoch 983/1000, Loss: 292.1811637878418\n",
      "Epoch 984/1000, Loss: 294.3566665649414\n",
      "Epoch 985/1000, Loss: 294.7799434661865\n",
      "Epoch 986/1000, Loss: 295.975191116333\n",
      "Epoch 987/1000, Loss: 296.69168853759766\n",
      "Epoch 988/1000, Loss: 294.73874282836914\n",
      "Epoch 989/1000, Loss: 295.80767822265625\n",
      "Epoch 990/1000, Loss: 292.3404769897461\n",
      "Epoch 991/1000, Loss: 296.32645416259766\n",
      "Epoch 992/1000, Loss: 293.7346382141113\n",
      "Epoch 993/1000, Loss: 295.42288970947266\n",
      "Epoch 994/1000, Loss: 293.5092010498047\n",
      "Epoch 995/1000, Loss: 294.14342498779297\n",
      "Epoch 996/1000, Loss: 290.90941047668457\n",
      "Epoch 997/1000, Loss: 294.4795608520508\n",
      "Epoch 998/1000, Loss: 295.9072074890137\n",
      "Epoch 999/1000, Loss: 295.53266525268555\n",
      "Epoch 1000/1000, Loss: 291.3500862121582\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=1000):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss (MSE): 231.6773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Use this for CNN only\\n# Function to evaluate the model on the test set\\ndef evaluate_model(model, test_loader):\\n    model.eval()\\n    actuals = []\\n    predictions = []\\n    with torch.no_grad():\\n        for inputs, targets in test_loader:\\n            outputs = model(inputs)\\n            actuals.append(targets)\\n            predictions.append(outputs)\\n    \\n    actuals = torch.cat(actuals).numpy().squeeze()  # Flatten to 1D\\n    predictions = torch.cat(predictions).numpy().squeeze()  # Flatten to 1D\\n    return actuals, predictions\\n\\n# Get the actual and predicted values\\nactuals, predictions = evaluate_model(model, test_loader)\\n\\n\\n# Plot actual vs predicted values\\nplt.figure(figsize=(8, 6))\\nplt.scatter(actuals, predictions, color=\\'blue\\', label=\\'Predicted vs Actual\\')\\nplt.plot([min(actuals), max(actuals)], [min(actuals), max(actuals)], color=\\'red\\', linestyle=\\'--\\', label=\\'Ideal Prediction\\')  # Line of perfect prediction\\nplt.xlabel(\"Actual E\\' (MPa)\")\\nplt.ylabel(\"Predicted E\\' (MPa)\")\\nplt.title(\"Predicted vs Actual E\\' Values for Convolutional Neural Network with LSTM (Hybrid)\")\\nplt.legend()\\nplt.grid(True)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions.append(outputs.numpy())\n",
    "            actuals.append(targets.numpy())\n",
    "    \n",
    "    return total_loss / len(test_loader), np.vstack(predictions), np.vstack(actuals)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, y_pred, y_true = evaluate(model, test_loader, criterion)\n",
    "print(f'Test Loss (MSE): {test_loss:.4f}')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Use this for CNN only\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            actuals.append(targets)\n",
    "            predictions.append(outputs)\n",
    "    \n",
    "    actuals = torch.cat(actuals).numpy().squeeze()  # Flatten to 1D\n",
    "    predictions = torch.cat(predictions).numpy().squeeze()  # Flatten to 1D\n",
    "    return actuals, predictions\n",
    "\n",
    "# Get the actual and predicted values\n",
    "actuals, predictions = evaluate_model(model, test_loader)\n",
    "\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(actuals, predictions, color='blue', label='Predicted vs Actual')\n",
    "plt.plot([min(actuals), max(actuals)], [min(actuals), max(actuals)], color='red', linestyle='--', label='Ideal Prediction')  # Line of perfect prediction\n",
    "plt.xlabel(\"Actual E' (MPa)\")\n",
    "plt.ylabel(\"Predicted E' (MPa)\")\n",
    "plt.title(\"Predicted vs Actual E' Values for Convolutional Neural Network with LSTM (Hybrid)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot Actual vs Predicted\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_true, y_pred, alpha=0.7)\n",
    "plt.xlabel(\"Actual E' (MPa)\")\n",
    "plt.ylabel(\"Predicted E' (MPa)\")\n",
    "plt.title(\"Actual vs Predicted E' for Bidirectional Long Short Term Memory (LSTM)\")\n",
    "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')  # Diagonal line\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
